{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment Five: Evaluation and Multi-Layer Perceptron\n",
    "## Rupal Sanghavi, Omar Roa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Case REDO THIS TO DO WITH MARKETING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset represents the responses from students and their friends(ages 15-30, henceforth stated as \"young people\") of a Statistics class from the Faculty of Social and Economic Sciences at The Comenius University in Bratislava, Slovakia. Their survey was a mix of various topics.\n",
    "\n",
    "* Music preferences (19 items)\n",
    "* Movie preferences (12 items)\n",
    "* Hobbies & interests (32 items)\n",
    "* Phobias (10 items)\n",
    "* Health habits (3 items)\n",
    "* Personality traits, views on life, & opinions (57 items)\n",
    "* Spending habits (7 items)\n",
    "* Demographics (10 items)\n",
    "\n",
    "The dataset can be found here. https://www.kaggle.com/miroslavsabo/young-people-survey\n",
    "\n",
    "Our target is to predict how likely a \"young person\" would be interested in shopping at a large shopping center. We were not given details about what a \"large\" shopping center, but searching online for malls led us to the Avion Shopping Park in Ru≈æinov, Slovakia. It has an area of 103,000m<sup>2</sup> and is the largest shopping mall in Slovakia (https://www.avion.sk/sk-sk/about-the-centre/fakty-a-cisla). \n",
    "\n",
    "Slovakia is in the lower half of European nations by size (28/48 - https://en.wikipedia.org/wiki/List_of_European_countries_by_area) and is very mountainous, making real estate space a precious commodity. This information would be of great interest to any commercial devlopment firm deciding on where to build their next shopping center or place of business. This could also help other parties trying to purchase real estate for youth-orientated construction (parks, recreation centers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline \n",
    "%load_ext memory_profiler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "import time\n",
    "import math\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "target_classifier = 'Shopping centres'\n",
    "df = pd.read_csv('responses.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Prepare Class Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove rows whose target classfier value is NaN\n",
    "df_cleaned_classifier = df[np.isfinite(df[target_classifier])]\n",
    "# change NaN number values to the mean\n",
    "df_imputed = df_cleaned_classifier.fillna(df.mean())\n",
    "# get categorical features\n",
    "object_features = list(df_cleaned_classifier.select_dtypes(include=['object']).columns)\n",
    "# one hot encode categorical features\n",
    "one_hot_df = pd.concat([pd.get_dummies(df_imputed[col],prefix=col) for col in object_features], axis=1)\n",
    "# drop object features from imputed dataframe\n",
    "df_imputed_dropped = df_imputed.drop(object_features, 1)\n",
    "frames = [df_imputed_dropped, one_hot_df]\n",
    "# concatenate both frames by columns\n",
    "df_fixed = pd.concat(frames, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned_classifier.isnull().sum().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1008, 173)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fixed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset (1010 rows and 150 columns) was mostly ordinal data as numbers (preferences ranked 1-5) . We also had some ordinal data as strings. \n",
    "\n",
    "e.g.\n",
    "How much time do you spend online?: No time at all - Less than an hour a day - Few hours a day - Most of the day\n",
    "\n",
    "We first removed any rows which contained NaN values for our target classifer, Shopping centres. Afterwards we imputed mean values for any NaN values in other features. We decided to impute due to the fact that there were not many NaN values in our features compared to the size of our data set. (At most was 20 for a feature, as shown above). We then one-hot encoded any string object, which created extra features.\n",
    "\n",
    "We are left with numerical values for our features and a size of 1008 x 173"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics To Evaluate Algorithm's Generalization Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Research on Cost Matrix\n",
    "# http://www.ibm.com/support/knowledgecenter/SSEPGG_11.1.0/com.ibm.im.model.doc/c_cost_matrix.html\n",
    "\n",
    "cost_matrix = np.matrix([[0,1,2,3,4],\n",
    "                        [1,0,1,2,3],\n",
    "                        [3,1,0,1,2],\n",
    "                        [5,3,1,0,1],\n",
    "                        [7,5,2,1,0]])\n",
    "\n",
    "def get_confusion_costTot(confusion_matrix, cost_matrix):\n",
    "    score = np.sum(confusion_matrix*cost_matrix)\n",
    "    return score\n",
    "\n",
    "confusion_scorer = make_scorer(get_confusion_costTot, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert explanation of scorer here here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide Data into Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using Stratified Shuffle Splits so we can get a good random mix of training and testing data. We want to split and randommize our data to prevent overfitting our model. Our split uses a 20/80 split for testing and training. This gives us a significant representation of data for training and testing. Upon further research, splitting our testing data into testing and validation seems further appropriate, but we are not implementing that for our work.\n",
    "\n",
    "Further down in our work, we are using Principal Component Analysis (via a Pipeline object) to reduce the number of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=10, random_state=None, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if target_classifier in df_fixed:\n",
    "    y = df_fixed[target_classifier].values # get the label we want\n",
    "    del df_fixed[target_classifier] # get rid of the class label\n",
    "    X = df_fixed.values # use everything else to predict!\n",
    "\n",
    "num_folds = 10\n",
    "\n",
    "cv_object = StratifiedKFold(n_splits= num_folds, random_state=None, shuffle=True)\n",
    "cv_object.split(X,y)\n",
    "\n",
    "print(cv_object)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
