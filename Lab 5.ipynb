{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment Five: Evaluation and Multi-Layer Perceptron\n",
    "## Rupal Sanghavi, Omar Roa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline \n",
    "%load_ext memory_profiler\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.special import expit\n",
    "import time\n",
    "import math\n",
    "from memory_profiler import memory_usage\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "target_classifier = 'Shopping centres'\n",
    "df = pd.read_csv('responses.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove rows whose target classfier value is NaN\n",
    "df_cleaned_classifier = df[np.isfinite(df[target_classifier])]\n",
    "# change NaN number values to the mean\n",
    "df_imputed = df_cleaned_classifier.fillna(df.mean())\n",
    "# get categorical features\n",
    "object_features = list(df_cleaned_classifier.select_dtypes(include=['object']).columns)\n",
    "# one hot encode categorical features\n",
    "one_hot_df = pd.concat([pd.get_dummies(df_imputed[col],prefix=col) for col in object_features], axis=1)\n",
    "# drop object features from imputed dataframe\n",
    "df_imputed_dropped = df_imputed.drop(object_features, 1)\n",
    "frames = [df_imputed_dropped, one_hot_df]\n",
    "# concatenate both frames by columns\n",
    "df_fixed = pd.concat(frames, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics To Evaluate Algorithm's Generalization Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_scorer(get_confusion_costTot, greater_is_better=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Research on Cost Matrix\n",
    "# http://www.ibm.com/support/knowledgecenter/SSEPGG_11.1.0/com.ibm.im.model.doc/c_cost_matrix.html\n",
    "\n",
    "cost_matrix = np.matrix([[0,1,2,3,4],\n",
    "[1,0,1,2,3],\n",
    "[3,1,0,1,2],\n",
    "[5,3,1,0,1],\n",
    "[7,5,2,1,0]])\n",
    "\n",
    "def get_confusion_costTot(confusion_matrix, cost_matrix):\n",
    "    score = np.sum(confusion_matrix*cost_matrix)\n",
    "    return score\n",
    "\n",
    "confusion_scorer = make_scorer(get_confusion_costTot, greater_is_better=False)\n",
    "confusion_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Implementation of Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=10, random_state=None, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if target_classifier in df_fixed:\n",
    "    y = df_fixed[target_classifier].values # get the label we want\n",
    "    del df_fixed[target_classifier] # get rid of the class label\n",
    "    X = df_fixed.values # use everything else to predict!\n",
    "\n",
    "X = X/5\n",
    "num_folds = 10\n",
    "\n",
    "cv_object = StratifiedKFold(n_splits= num_folds, random_state=None, shuffle=True)\n",
    "cv_object.split(X,y)\n",
    "\n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  0.6,  0.4, ...,  0.2,  0.2,  0. ],\n",
       "       [ 0.8,  0.8,  0.4, ...,  0. ,  0.2,  0. ],\n",
       "       [ 1. ,  1. ,  0.4, ...,  0. ,  0.2,  0. ],\n",
       "       ..., \n",
       "       [ 0.8,  0.6,  0.2, ...,  0. ,  0.2,  0. ],\n",
       "       [ 1. ,  0.6,  0.6, ...,  0. ,  0.2,  0. ],\n",
       "       [ 1. ,  1. ,  0.8, ...,  0.2,  0. ,  0.2]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
    "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None, nonlinearity = \"sigmoid\"):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.params = {}\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0,size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "        \n",
    "        W2_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden + 1)\n",
    "        return W1, W2\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _feedforward(self, X, W1, W2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        \"\"\"\n",
    "        A1 = self._add_bias_unit(X, how='column')\n",
    "        Z1 = W1 @ A1.T\n",
    "        if(self.nonlinearity == \"sigmoid\"):\n",
    "            A2 = self._sigmoid(Z1)\n",
    "            A2 = self._add_bias_unit(A2, how='row')\n",
    "            Z2 = W2 @ A2\n",
    "            A3 = self._sigmoid(Z2)\n",
    "        else:\n",
    "            A2 = Z1\n",
    "            A2 = self._add_bias_unit(A2, how='row')\n",
    "            Z2 = W2 @ A2\n",
    "            A3 = Z2\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        if(self.nonlinearity == \"sigmoid\"):\n",
    "            sigma3 = -2*(Y_enc-A3)*A3*(1-A3)\n",
    "            sigma2 = (W2.T @ sigma3)*A2*(1-A2)\n",
    "        else:\n",
    "            sigma3 = -2*(Y_enc-A3)\n",
    "            sigma2 = (W2.T @ sigma3)\n",
    "            \n",
    "        grad1 = sigma2[1:,:] @ A1\n",
    "        grad2 = sigma3 @ A2.T\n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "    def get_params(self,deep=False):\n",
    "        return dict(n_hidden=self.n_hidden, C=self.l2_C, nonlinearity=self.nonlinearity)\n",
    "\n",
    "    def set_params(self,**kwds):\n",
    "        print(kwds)\n",
    "        self.n_hidden = kwds['n_hidden']\n",
    "        self.C = kwds['C']\n",
    "        self.nonlinearity = kwds['nonlinearity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# just start with the vectorized version and minibatch\n",
    "class TLPMiniBatch(TwoLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.0, shuffle=True, \n",
    "                 minibatches=1, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 = self._initialize_weights()\n",
    "\n",
    "        delta_W1_prev = np.zeros(self.W1.shape)\n",
    "        delta_W2_prev = np.zeros(self.W2.shape)\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3 = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2)\n",
    "                \n",
    "                cost = self._cost(A3,Y_enc[:, idx],self.W1,self.W2)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, \n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=self.W1,W2=self.W2)\n",
    "\n",
    "                delta_W1, delta_W2 = self.eta * grad1, self.eta * grad2\n",
    "                self.W1 -= (delta_W1 + (self.alpha * delta_W1_prev))\n",
    "                self.W2 -= (delta_W2 + (self.alpha * delta_W2_prev))\n",
    "                delta_W1_prev, delta_W2_prev = delta_W1, delta_W2\n",
    "\n",
    "            self.cost_.append(mini_cost)\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            \n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to implement the new style of objective function, \n",
    "# we just need to update the final layer calculation of the gradient\n",
    "class TLPMiniBatchCrossEntropy(TLPMiniBatch):\n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        sigma3 = (A3-Y_enc) # <- this is only line that changed\n",
    "        if(self.nonlinearity == \"sigmoid\"):\n",
    "            sigma2 = (W2.T @ sigma3)*A2*(1-A2)\n",
    "            grad1 = sigma2[1:,:] @ A1\n",
    "            grad2 = sigma3 @ A2.T\n",
    "        else:\n",
    "            sigma2 = (W2.T @ sigma3)\n",
    "            grad1 = sigma2[1:,:] @ A1\n",
    "            grad2 = sigma3\n",
    "        #grad1 = sigma2[1:,:] @ A1\n",
    "        #grad2 = sigma3 @ A2.T\n",
    "        grad2 = sigma3 @ A2.T\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TLPDropoutQuad(TLPMiniBatch):\n",
    "    def __init__(self, dropout=True, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.dropout = dropout\n",
    "\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "        \n",
    "    def fit(self, X, y, print_progress=0, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 = self._initialize_weights()\n",
    "\n",
    "        delta_W1_prev = np.zeros(self.W1.shape)\n",
    "        delta_W2_prev = np.zeros(self.W2.shape)\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            \n",
    "            # adding dropout neurons\n",
    "            W1 = self.W1.copy()\n",
    "            W2 = self.W2.copy()\n",
    "            \n",
    "            if self.dropout:\n",
    "                # be sure to select the other half of the neurons each epoch\n",
    "                if True :#i%2 == 0:\n",
    "                    # randomly select half of the neurons\n",
    "                    idx_dropout = np.random.permutation(W1.shape[0])\n",
    "                    idx_other_half = idx_dropout[:int(W1.shape[0]/2)]\n",
    "                    idx_dropout = idx_dropout[int(W1.shape[0]/2):] #drop half\n",
    "                else:\n",
    "                    # select the other half\n",
    "                    idx_dropout = idx_other_half\n",
    "                    \n",
    "                idx_dropout = np.sort(idx_dropout)\n",
    "                idx_W2_withbias = np.hstack(([0],(idx_dropout+1)))\n",
    "                W1 = W1[idx_dropout,:]# get rid of rows\n",
    "                W2 = W2[:,idx_W2_withbias]# get rid of extra columns\n",
    "                delta_W1_prev_dropout = delta_W1_prev[idx_dropout,:]\n",
    "                delta_W2_prev_dropout = delta_W2_prev[:,idx_W2_withbias]\n",
    "            else:\n",
    "                delta_W1_prev_dropout = delta_W1_prev\n",
    "                delta_W2_prev_dropout = delta_W2_prev\n",
    "                \n",
    "            \n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3 = self._feedforward(X_data[idx],\n",
    "                                                       W1,\n",
    "                                                       W2)\n",
    "                \n",
    "                cost = self._cost(A3,Y_enc[:, idx],W1,W2)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2,\n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=W1,W2=W2)\n",
    "\n",
    "                delta_W1, delta_W2 = self.eta * grad1, self.eta * grad2\n",
    "                W1 -= (delta_W1 + (self.alpha * delta_W1_prev_dropout))\n",
    "                W2 -= (delta_W2 + (self.alpha * delta_W2_prev_dropout))\n",
    "                delta_W1_prev_dropout, delta_W2_prev_dropout = delta_W1, delta_W2\n",
    "\n",
    "            if self.dropout:\n",
    "                # now append the learned weights back into the original matrices\n",
    "                self.W1[idx_dropout,:] = W1\n",
    "                self.W2[:,idx_W2_withbias] = W2\n",
    "                delta_W1_prev[idx_dropout,:] = delta_W1_prev_dropout\n",
    "                delta_W2_prev[:,idx_W2_withbias] = delta_W2_prev_dropout\n",
    "            else:\n",
    "                # don't eliminate any neurons\n",
    "                self.W1 = W1\n",
    "                self.W2 = W2\n",
    "                delta_W1_prev = delta_W1_prev_dropout\n",
    "                delta_W2_prev = delta_W2_prev_dropout\n",
    "                \n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            self.cost_.append(mini_cost) # only uses dropped samples, so more noise\n",
    "            if XY_test is not None:\n",
    "                self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class TLPDropout(TLPMiniBatchCrossEntropy):\n",
    "    def __init__(self, dropout=True, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.dropout = dropout\n",
    "\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "        \n",
    "    def fit(self, X, y, print_progress=0, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 = self._initialize_weights()\n",
    "\n",
    "        delta_W1_prev = np.zeros(self.W1.shape)\n",
    "        delta_W2_prev = np.zeros(self.W2.shape)\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            \n",
    "            # adding dropout neurons\n",
    "            W1 = self.W1.copy()\n",
    "            W2 = self.W2.copy()\n",
    "            \n",
    "            if self.dropout:\n",
    "                # be sure to select the other half of the neurons each epoch\n",
    "                if True :#i%2 == 0:\n",
    "                    # randomly select half of the neurons\n",
    "                    idx_dropout = np.random.permutation(W1.shape[0])\n",
    "                    idx_other_half = idx_dropout[:int(W1.shape[0]/2)]\n",
    "                    idx_dropout = idx_dropout[int(W1.shape[0]/2):] #drop half\n",
    "                else:\n",
    "                    # select the other half\n",
    "                    idx_dropout = idx_other_half\n",
    "                    \n",
    "                idx_dropout = np.sort(idx_dropout)\n",
    "                idx_W2_withbias = np.hstack(([0],(idx_dropout+1)))\n",
    "                W1 = W1[idx_dropout,:]# get rid of rows\n",
    "                W2 = W2[:,idx_W2_withbias]# get rid of extra columns\n",
    "                delta_W1_prev_dropout = delta_W1_prev[idx_dropout,:]\n",
    "                delta_W2_prev_dropout = delta_W2_prev[:,idx_W2_withbias]\n",
    "            else:\n",
    "                delta_W1_prev_dropout = delta_W1_prev\n",
    "                delta_W2_prev_dropout = delta_W2_prev\n",
    "                \n",
    "            \n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3 = self._feedforward(X_data[idx],\n",
    "                                                       W1,\n",
    "                                                       W2)\n",
    "                \n",
    "                cost = self._cost(A3,Y_enc[:, idx],W1,W2)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2,\n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=W1,W2=W2)\n",
    "\n",
    "                delta_W1, delta_W2 = self.eta * grad1, self.eta * grad2\n",
    "                W1 -= (delta_W1 + (self.alpha * delta_W1_prev_dropout))\n",
    "                W2 -= (delta_W2 + (self.alpha * delta_W2_prev_dropout))\n",
    "                delta_W1_prev_dropout, delta_W2_prev_dropout = delta_W1, delta_W2\n",
    "\n",
    "            if self.dropout:\n",
    "                # now append the learned weights back into the original matrices\n",
    "                self.W1[idx_dropout,:] = W1\n",
    "                self.W2[:,idx_W2_withbias] = W2\n",
    "                delta_W1_prev[idx_dropout,:] = delta_W1_prev_dropout\n",
    "                delta_W2_prev[:,idx_W2_withbias] = delta_W2_prev_dropout\n",
    "            else:\n",
    "                # don't eliminate any neurons\n",
    "                self.W1 = W1\n",
    "                self.W2 = W2\n",
    "                delta_W1_prev = delta_W1_prev_dropout\n",
    "                delta_W2_prev = delta_W2_prev_dropout\n",
    "                \n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            self.cost_.append(mini_cost) # only uses dropped samples, so more noise\n",
    "            if XY_test is not None:\n",
    "                self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TLPGaussianInitialQuad(TLPMiniBatch):             \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with smal l random numbers.\"\"\"\n",
    "        W1 = np.random.randn(self.n_hidden, self.n_features_ + 1)\n",
    "        W1[:,1:] = W1[:,1:]/np.sqrt(self.n_features_+1) # don't saturate the neuron\n",
    "        \n",
    "        W2 = np.random.randn(self.n_output_, self.n_hidden + 1)\n",
    "        W2[:,1:] = W2[:,1:]/np.sqrt(self.n_hidden+1) # don't saturate the neuron\n",
    "        return W1, W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TLPGaussianInitial(TLPMiniBatchCrossEntropy):             \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1 = np.random.randn(self.n_hidden, self.n_features_ + 1)\n",
    "        W1[:,1:] = W1[:,1:]/np.sqrt(self.n_features_+1) # don't saturate the neuron\n",
    "        \n",
    "        W2 = np.random.randn(self.n_output_, self.n_hidden + 1)\n",
    "        W2[:,1:] = W2[:,1:]/np.sqrt(self.n_hidden+1) # don't saturate the neuron\n",
    "        return W1, W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vals = {'n_hidden':50, \n",
    "         'C':1e-2, 'epochs':75, 'eta':0.001, \n",
    "         'alpha':0.0, 'decrease_const':1e-9, 'minibatches':200,\n",
    "         'shuffle':True,'random_state':1, 'dropout':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TLPReLu(TLPDropout):\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        # suggested relu/sigmoid bounds\n",
    "        # Glorot, Xavier, Antoine Bordes, and Yoshua Bengio. \n",
    "        #   \"Deep Sparse Rectifier Neural Networks.\"\n",
    "        init_bound = np.sqrt(6. / (self.n_hidden + self.n_features_ + 1))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_ + 1))\n",
    "\n",
    "        init_bound = np.sqrt(2. / (self.n_output_ + self.n_hidden + 1))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden + 1))\n",
    "        return W1, W2\n",
    "    \n",
    "    @staticmethod\n",
    "    def _relu(Z):\n",
    "        return np.maximum(0,Z.copy())\n",
    "        \n",
    "    def _feedforward(self, X, W1, W2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        \"\"\"\n",
    "        # A1->W1->ReLu->A2->W2->Sigmoid\n",
    "        A1 = self._add_bias_unit(X, how='column')\n",
    "        Z1 = W1 @ A1.T\n",
    "        A2 = self._relu(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        sigma3 = (A3-Y_enc) \n",
    "        # sigma3[Z2<=0] = 0 # can change to be relu back prop on this layer too!\n",
    "        \n",
    "        sigma2 = (W2.T @ sigma3) \n",
    "        Z1_with_bias = self._add_bias_unit(Z1,how='row')\n",
    "        sigma2[Z1_with_bias<=0] = 0\n",
    "        # relu derivative only zeros out certain values! easy!\n",
    "        \n",
    "        grad1 = sigma2[1:,:] @ A1\n",
    "        grad2 = sigma3 @ A2.T\n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += (W1[:, 1:] * self.l2_C)\n",
    "        grad2[:, 1:] += (W2[:, 1:] * self.l2_C)\n",
    "\n",
    "        return grad1, grad2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def print_result(nn,X_train,y_train,X_test,y_test,title=\"\",color=\"red\"):\n",
    "    \n",
    "    print(\"=================\")\n",
    "    print(title,\":\")\n",
    "    yhat = nn.predict(X_train)\n",
    "    print('Resubstitution acc:',accuracy_score(y_train,yhat))\n",
    "    \n",
    "    yhat = nn.predict(X_test)\n",
    "    print('Validation acc:',accuracy_score(y_test,yhat))\n",
    "    \n",
    "    if hasattr(nn,'val_score_'):\n",
    "        plt.plot(range(len(nn.val_score_)), nn.val_score_, color=color,label=title)\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "    else:\n",
    "        plt.plot(range(len(nn.score_)), nn.score_, color=color,label=title)\n",
    "        plt.ylabel('Resub Accuracy')\n",
    "        \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3/75"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "\n",
      "quadratic\n",
      "sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5/75"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.3 s, sys: 221 ms, total: 14.5 s\n",
      "Wall time: 3.68 s\n",
      "confusion matrix\n",
      " [[ 0  0  2 12  0]\n",
      " [ 0  0  2 14  2]\n",
      " [ 0  1  3 17  4]\n",
      " [ 0  1  0 15  8]\n",
      " [ 0  0  0 13  9]]\n",
      "Weighted Confusion Matrix Score:  1118\n",
      "---------------------------------\n",
      "\n",
      "quadratic\n",
      "sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4/75"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.6 s, sys: 334 ms, total: 15.9 s\n",
      "Wall time: 4.04 s\n",
      "confusion matrix\n",
      " [[ 0  0 14  0  0]\n",
      " [ 0  0 17  1  0]\n",
      " [ 0  0 25  0  0]\n",
      " [ 0  0 22  2  0]\n",
      " [ 0  0 18  4  0]]\n",
      "Weighted Confusion Matrix Score:  742\n",
      "---------------------------------\n",
      "\n",
      "quadratic\n",
      "sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4/75"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.2 s, sys: 296 ms, total: 16.5 s\n",
      "Wall time: 4.16 s\n",
      "confusion matrix\n",
      " [[ 0  0 12  0  1]\n",
      " [ 0  0 10  0  8]\n",
      " [ 0  1 11  0 13]\n",
      " [ 0  0 11  0 13]\n",
      " [ 0  0  5  0 17]]\n",
      "Weighted Confusion Matrix Score:  1130\n",
      "---------------------------------\n",
      "\n",
      "quadratic\n",
      "sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5/75"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.7 s, sys: 411 ms, total: 16.1 s\n",
      "Wall time: 4.12 s\n",
      "confusion matrix\n",
      " [[ 0  0 10  2  1]\n",
      " [ 0  0 14  3  1]\n",
      " [ 0  0 14  4  6]\n",
      " [ 0  0 12  0 12]\n",
      " [ 0  0  6  0 16]]\n",
      "Weighted Confusion Matrix Score:  1022\n",
      "---------------------------------\n",
      "\n",
      "quadratic\n",
      "sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4/75"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.8 s, sys: 234 ms, total: 15 s\n",
      "Wall time: 3.78 s\n",
      "confusion matrix\n",
      " [[ 0  2  7  3  1]\n",
      " [ 0  0 12  6  0]\n",
      " [ 0  0 15  4  5]\n",
      " [ 0  0  5 10  9]\n",
      " [ 0  0  2  9 11]]\n",
      "Weighted Confusion Matrix Score:  1011\n",
      "---------------------------------\n",
      "\n",
      "quadratic\n",
      "sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5/75"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.6 s, sys: 192 ms, total: 14.8 s\n",
      "Wall time: 3.73 s\n",
      "confusion matrix\n",
      " [[ 0  0 13  0  0]\n",
      " [ 0  1 17  0  0]\n",
      " [ 0  0 24  0  0]\n",
      " [ 0  0 22  0  2]\n",
      " [ 0  0 15  2  5]]\n",
      "Weighted Confusion Matrix Score:  769\n",
      "---------------------------------\n",
      "\n",
      "quadratic\n",
      "sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5/75"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.4 s, sys: 184 ms, total: 14.6 s\n",
      "Wall time: 3.66 s\n",
      "confusion matrix\n",
      " [[ 0  0 10  1  2]\n",
      " [ 0  0 16  0  1]\n",
      " [ 0  0 21  1  2]\n",
      " [ 0  0 16  2  6]\n",
      " [ 0  0 12  2  8]]\n",
      "Weighted Confusion Matrix Score:  870\n",
      "---------------------------------\n",
      "\n",
      "quadratic\n",
      "sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4/75"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 s, sys: 299 ms, total: 15.6 s\n",
      "Wall time: 3.95 s\n",
      "confusion matrix\n",
      " [[ 0  0  9  2  2]\n",
      " [ 0  0 11  2  4]\n",
      " [ 0  0 11  1 12]\n",
      " [ 0  1  5  5 13]\n",
      " [ 0  0  3  2 17]]\n",
      "Weighted Confusion Matrix Score:  1120\n",
      "---------------------------------\n",
      "\n",
      "quadratic\n",
      "sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5/75"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.2 s, sys: 214 ms, total: 15.4 s\n",
      "Wall time: 3.87 s\n",
      "confusion matrix\n",
      " [[ 0  2  8  3  0]\n",
      " [ 0  0  9  7  1]\n",
      " [ 0  0 12 10  2]\n",
      " [ 0  0  9  9  5]\n",
      " [ 0  0  8 12  2]]\n",
      "Weighted Confusion Matrix Score:  896\n",
      "---------------------------------\n",
      "\n",
      "quadratic\n",
      "sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/75/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:62: RuntimeWarning: overflow encountered in square\n",
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:61: RuntimeWarning: invalid value encountered in multiply\n",
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:62: RuntimeWarning: invalid value encountered in multiply\n",
      "Epoch: 4/75"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.5 s, sys: 274 ms, total: 15.8 s\n",
      "Wall time: 3.98 s\n",
      "confusion matrix\n",
      " [[ 0  0  6  1  6]\n",
      " [ 0  1  8  2  6]\n",
      " [ 0  0  5  2 17]\n",
      " [ 0  0 11  2 10]\n",
      " [ 0  0  5  2 14]]\n",
      "Weighted Confusion Matrix Score:  1137\n",
      "---------------------------------\n",
      "\n",
      "quadratic\n",
      "linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/75/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:62: RuntimeWarning: overflow encountered in square\n",
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:62: RuntimeWarning: invalid value encountered in multiply\n",
      "Epoch: 5/75"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.3 s, sys: 279 ms, total: 14.6 s\n",
      "Wall time: 3.69 s\n",
      "confusion matrix\n",
      " [[14  0  0  0  0]\n",
      " [18  0  0  0  0]\n",
      " [25  0  0  0  0]\n",
      " [24  0  0  0  0]\n",
      " [22  0  0  0  0]]\n",
      "Weighted Confusion Matrix Score:  1030\n",
      "---------------------------------\n",
      "\n",
      "quadratic\n",
      "linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/75/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:62: RuntimeWarning: overflow encountered in square\n",
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:62: RuntimeWarning: invalid value encountered in multiply\n",
      "Epoch: 5/75"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.8 s, sys: 242 ms, total: 14 s\n",
      "Wall time: 3.53 s\n",
      "confusion matrix\n",
      " [[14  0  0  0  0]\n",
      " [18  0  0  0  0]\n",
      " [25  0  0  0  0]\n",
      " [24  0  0  0  0]\n",
      " [22  0  0  0  0]]\n",
      "Weighted Confusion Matrix Score:  1030\n",
      "---------------------------------\n",
      "\n",
      "quadratic\n",
      "linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/75/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:62: RuntimeWarning: overflow encountered in square\n",
      "/Users/rupalsanghavi/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:62: RuntimeWarning: invalid value encountered in multiply\n",
      "Epoch: 5/75"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 s, sys: 181 ms, total: 13.6 s\n",
      "Wall time: 3.41 s\n",
      "confusion matrix\n",
      " [[13  0  0  0  0]\n",
      " [18  0  0  0  0]\n",
      " [25  0  0  0  0]\n",
      " [24  0  0  0  0]\n",
      " [22  0  0  0  0]]\n",
      "Weighted Confusion Matrix Score:  1020\n",
      "---------------------------------\n",
      "\n",
      "quadratic\n",
      "linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 51/75"
     ]
    }
   ],
   "source": [
    "with np.errstate(all='ignore'):\n",
    "\n",
    "    nonlinearities = [\"sigmoid\",\"linear\"]\n",
    "    costs = ['quadratic','cross']\n",
    "    \n",
    "    custom_performances = []\n",
    "    lowest_score = 10000\n",
    "    best_cost = \"\"\n",
    "    best_nonlin = \"\"\n",
    "for cost in costs:\n",
    "    for nonlinearity in nonlinearities:\n",
    "\n",
    "        vals = {'n_hidden':50, \n",
    "                     'C':1e-2, 'epochs':75, 'eta':0.001, \n",
    "                     'alpha':0.0, 'decrease_const':1e-9, 'minibatches':200,\n",
    "                     'shuffle':True,'random_state':1, \n",
    "                       'nonlinearity': nonlinearity}\n",
    "        for train_indices, test_indices in cv_object.split(X,y): \n",
    "                    print(\"---------------------------------\")\n",
    "                    print(\"\")\n",
    "                    print(cost)\n",
    "                    print(nonlinearity)\n",
    "                    # I will create new variables here so that it is more obvious what \n",
    "                    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "                    # but it makes this code less readable)\n",
    "                    X_train = (X[train_indices])\n",
    "                    y_train = y[train_indices]\n",
    "\n",
    "                    X_test = (X[test_indices])\n",
    "                    y_test = y[test_indices]\n",
    "                    \n",
    "                    if(cost == \"quadratic\"):\n",
    "                        nn_long_sigmoid = TLPGaussianInitialQuad(**vals)\n",
    "                    else:\n",
    "                        print(\"in\")\n",
    "                        nn_long_sigmoid = TLPGaussianInitial(**vals)\n",
    "\n",
    "                    #%time nn_long_sigmoid.fit(X_train, y_train, print_progress=1, XY_test=(X_test,y_test))\n",
    "                    %time nn_long_sigmoid.fit(X_train, y_train, print_progress=1)\n",
    "                    y_hat = nn_long_sigmoid.predict(X_test) # get test set precitions\n",
    "\n",
    "                    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "                    acc = mt.accuracy_score(y_test,y_hat+1)\n",
    "            #         lr_clf_accuracies.append(acc)\n",
    "            #         cost_accuracies.append([acc])\n",
    "\n",
    "                    conf = mt.confusion_matrix(y_test,y_hat+1)\n",
    "        #             print(vals)\n",
    "#                     print_result(nn_long_sigmoid,X_train,y_train,X_test,y_test,title=\"Long Run\",color=\"red\")\n",
    "#                     plt.show()\n",
    "                    print(\"confusion matrix\\n\",conf)\n",
    "                    score = get_confusion_costTot(conf, cost_matrix)\n",
    "                    print(\"Weighted Confusion Matrix Score: \", score)\n",
    "                    custom_performances.append(score)\n",
    "                    if(score < lowest_score): \n",
    "                        lowest_score=score\n",
    "                        best_cost=cost\n",
    "                        best_nonlin=nonlinearity\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "print(\"Best Score: \",lowest_score)               \n",
    "print(\"Best cost: \",best_cost)\n",
    "print(\"Best Non-Linearity: \",best_nonlin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_neurons = np.linspace(150, 200, num=10)\n",
    "hidden_neurons.sort()\n",
    "\n",
    "costs = np.logspace(-3,1, num=10)\n",
    "costs.sort()\n",
    "\n",
    "nonlinearities = [\"sigmoid\",\"linear\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with np.errstate(all='ignore'):\n",
    "    param_grid_input = {'n_hidden': hidden_neurons, 'C': costs, 'nonlinearity' : nonlinearities}\n",
    "    gscv = GridSearchCV(cv= cv_object, estimator=nn_long_sigmoid, param_grid= param_grid_input, scoring= confusion_scorer,refit=False)\n",
    "    gscv.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing our MLP Implementation with that of Scikit Learn  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vals = {'n_hidden':50, \n",
    "         'C':1e-2, 'epochs':15, 'eta':0.001, \n",
    "         'alpha':0.0, 'decrease_const':1e-9, 'minibatches':200,\n",
    "         'shuffle':True,'random_state':1, \n",
    "           'nonlinearity': \"sigmoid\"}\n",
    "custom_performances = []\n",
    "custom_times = []\n",
    "custom_mem = []\n",
    "sk_performances = []\n",
    "sk_times = []\n",
    "sk_mem = []\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "            \n",
    "            # I will create new variables here so that it is more obvious what \n",
    "            # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "            # but it makes this code less readable)\n",
    "            X_train = (X[train_indices])\n",
    "            y_train = y[train_indices]\n",
    "\n",
    "            X_test = (X[test_indices])\n",
    "            y_test = y[test_indices]\n",
    "\n",
    "            nn_long_sigmoid = TLPGaussianInitialQuad(**vals)\n",
    "           \n",
    "\n",
    "            #%time nn_long_sigmoid.fit(X_train, y_train, print_progress=1, XY_test=(X_test,y_test))\n",
    "            st = time.time()\n",
    "\n",
    "            mem = memory_usage((nn_long_sigmoid.fit,(X_train,y_train))) # train object\n",
    "            t = (time.time() -st)\n",
    "            custom_times.append(t)\n",
    "            custom_mem.append(mem[0])\n",
    "\n",
    "            %time nn_long_sigmoid.fit(X_train, y_train, print_progress=1)\n",
    "            y_hat = nn_long_sigmoid.predict(X_test) # get test set precitions\n",
    "\n",
    "            # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "            acc = mt.accuracy_score(y_test,y_hat+1)\n",
    "    #         lr_clf_accuracies.append(acc)\n",
    "    #         cost_accuracies.append([acc])\n",
    "\n",
    "            conf = mt.confusion_matrix(y_test,y_hat+1)\n",
    "#             print(vals)\n",
    "#                     print_result(nn_long_sigmoid,X_train,y_train,X_test,y_test,title=\"Long Run\",color=\"red\")\n",
    "#                     plt.show()\n",
    "            print(\"confusion matrix\\n\",conf)\n",
    "            score = get_confusion_costTot(conf, cost_matrix)\n",
    "            print(\"Weighted Confusion Matrix Score: \", score)\n",
    "            custom_performances.append(score)\n",
    "            \n",
    "            clf = MLPClassifier(hidden_layer_sizes=(50, ), \n",
    "                        activation='relu', # type of non-linearity, every layer\n",
    "                        solver='sgd', \n",
    "                        alpha=1e-4, # L2 penalty\n",
    "                        batch_size= 'auto', # min of 200, num_samples\n",
    "                        learning_rate='constant', # adapt learning? only for sgd\n",
    "                        learning_rate_init=0.1, # only SGD\n",
    "                        power_t=0.0,    # only SGD with inverse scaling of learning rate\n",
    "                        max_iter=75, # stopping criteria\n",
    "                        shuffle=True, \n",
    "                        random_state=1, \n",
    "                        tol=0, # for stopping\n",
    "                        verbose=False, \n",
    "                        warm_start=False, \n",
    "                        momentum=0.9, # only SGD\n",
    "                        nesterovs_momentum=False, # only SGD\n",
    "                        early_stopping=False, \n",
    "                        validation_fraction=0.0, # only if early_stop is true\n",
    "                        beta_1=0.9, # adam decay rate of moment\n",
    "                        beta_2=0.999, # adam decay rate of moment\n",
    "                        epsilon=1e-08) # adam numerical stabilizer\n",
    "            \n",
    "            print(\"SCIKIT*****\")\n",
    "            \n",
    "            st = time.time()\n",
    "\n",
    "            mem = memory_usage((clf.fit,(X_train,y_train))) # train object\n",
    "            t = (time.time() -st)\n",
    "            sk_times.append(t)\n",
    "            sk_mem.append(mem[0])\n",
    "#             %time clf.fit(X_train,y_train)\n",
    "            yhat = clf.predict(X_test)\n",
    "            print('Validation Acc:',accuracy_score(yhat,y_test))\n",
    "            conf = mt.confusion_matrix(y_test,y_hat)\n",
    "                    #             print(vals)\n",
    "            #                     print_result(nn_long_sigmoid,X_train,y_train,X_test,y_test,title=\"Long Run\",color=\"red\")\n",
    "            #                     plt.show()\n",
    "            print(\"confusion matrix\\n\",conf)\n",
    "            score = get_confusion_costTot(conf, cost_matrix)\n",
    "            print(\"Weighted Confusion Matrix Score: \", score)\n",
    "            sk_performances.append(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Implementations in terms of Generalization Performance, Computation Time, and Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sk_performances)\n",
    "print(custom_performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.boxplot([sk_performances,custom_performances])\n",
    "plt.title(\"Comparing Generalization Perfomances\")\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Generalization Performances')\n",
    "plt.xticks([1,2],['SKL','OURS'])\n",
    "plt.figure()\n",
    "print((time.time() -st)*100)\n",
    "# ax = fig.add_subplot(111)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.boxplot([sk_times,custom_times])\n",
    "plt.title(\"Comparing Computation Times\")\n",
    "plt.xlabel('Implementation')\n",
    "plt.ylabel('Training Time (seconds) ')\n",
    "plt.xticks([1,2],['SKL','OURS'])\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.boxplot([sk_mem,custom_mem])\n",
    "plt.title(\"Comparing Memory \")\n",
    "plt.xlabel('Implementation ')\n",
    "plt.ylabel('Memory Usage (mb) ')\n",
    "plt.xticks([1,2],['SKL','OURS'])\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
