{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment Four: Extending Logistic Regression \n",
    "## Rupal Sanghavi, Omar Roa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset represents the responses from students and their friends(ages 15-30, henceforth stated as \"young people\") of a Statistics class from the Faculty of Social and Economic Sciences at The Comenius University in Bratislava, Slovakia. Their survey was a mix of various topics.\n",
    "\n",
    "* Music preferences (19 items)\n",
    "* Movie preferences (12 items)\n",
    "* Hobbies & interests (32 items)\n",
    "* Phobias (10 items)\n",
    "* Health habits (3 items)\n",
    "* Personality traits, views on life, & opinions (57 items)\n",
    "* Spending habits (7 items)\n",
    "* Demographics (10 items)\n",
    "\n",
    "The dataset can be found here. https://www.kaggle.com/miroslavsabo/young-people-survey\n",
    "\n",
    "Our target is to predict how likely a \"young person\" would be interested in shopping at a large shopping center. We were not given details about what a \"large\" shopping center, but searching online for malls led us to the Avion Shopping Park in Ružinov, Slovakia. It has an area of 103,000m<sup>2</sup> and is the largest shopping mall in Slovakia (https://www.avion.sk/sk-sk/about-the-centre/fakty-a-cisla). \n",
    "\n",
    "Slovakia is in the lower half of European nations by size (28/48 - https://en.wikipedia.org/wiki/List_of_European_countries_by_area) and is very mountainous, making real estate space a precious commodity. This information would be of great interest to any commercial devlopment firm deciding on where to build their next shopping center or place of business. This could also help other parties trying to purchase real estate for youth-orientated construction (parks, recreation centers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline \n",
    "%load_ext memory_profiler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "import time\n",
    "import math\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "target_classifier = 'Shopping centres'\n",
    "df = pd.read_csv('responses.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Prepare Class Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove rows whose target classfier value is NaN\n",
    "df_cleaned_classifier = df[np.isfinite(df[target_classifier])]\n",
    "# change NaN number values to the mean\n",
    "df_imputed = df_cleaned_classifier.fillna(df.mean())\n",
    "# get categorical features\n",
    "object_features = list(df_cleaned_classifier.select_dtypes(include=['object']).columns)\n",
    "# one hot encode categorical features\n",
    "one_hot_df = pd.concat([pd.get_dummies(df_imputed[col],prefix=col) for col in object_features], axis=1)\n",
    "# drop object features from imputed dataframe\n",
    "df_imputed_dropped = df_imputed.drop(object_features, 1)\n",
    "frames = [df_imputed_dropped, one_hot_df]\n",
    "# concatenate both frames by columns\n",
    "df_fixed = pd.concat(frames, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned_classifier.isnull().sum().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1008, 173)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fixed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset (1010 rows and 150 columns) was mostly ordinal data as numbers (preferences ranked 1-5) . We also had some ordinal data as strings. \n",
    "\n",
    "e.g.\n",
    "How much time do you spend online?: No time at all - Less than an hour a day - Few hours a day - Most of the day\n",
    "\n",
    "We first removed any rows which contained NaN values for our target classifer, Shopping centres. Afterwards we imputed mean values for any NaN values in other features. We decided to impute due to the fact that there were not many NaN values in our features compared to the size of our data set. (At most was 20 for a feature, as shown above). We then one-hot encoded any string object, which created extra features.\n",
    "\n",
    "We are left with numerical values for our features and a size of 1008 x 173"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide Data into Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if target_classifier in df_fixed:\n",
    "    y = df_fixed[target_classifier].values # get the labels we want\n",
    "    del df_fixed[target_classifier] # get rid of the class label\n",
    "    X = df_fixed.values # use everything else to predict!\n",
    "\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,test_size = 0.2)\n",
    "\n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using Stratified Shuffle Splits so we can get a good random mix of training and testing data. We want to split and randommize our data to prevent overfitting our model. Our split uses a 20/80 split for testing and training. This gives us a significant representation of data for training and testing. Upon further research, splitting our testing data into testing and validation seems further appropriate, but we are not implementing that for our work.\n",
    "\n",
    "Further down in our work, we are using Principal Component Analysis (via a Pipeline object) to reduce the number of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating One-Versus-All Logistic Regression Classifier\n",
    "### Note: Each class allows a parameter for L1, L2, or both to be chosen in its gradient method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54 µs, sys: 1e+03 ns, total: 55 µs\n",
      "Wall time: 58.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from last time, our logistic regression algorithm is given by (including everything we previously had):\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001,reg=0):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.iterations = 0\n",
    "        self.reg = reg\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if(self.reg == 0):\n",
    "            gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        elif(self.reg == 1):\n",
    "            gradient[1:] += np.sign(self.w_[1:]) * self.C\n",
    "        else:\n",
    "            gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "            gradient[1:] += np.sign(self.w_[1:]) * self.C\n",
    "        #gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    def _get_l1_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += np.sign(self.w_[1:]) * self.C\n",
    "        return gradient\n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "# blr = BinaryLogisticRegression(eta=0.1,iterations=500,C=0.001)\n",
    "\n",
    "# blr.fit(X,y)\n",
    "# print(blr)\n",
    "\n",
    "# yhat = blr.predict(X)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 61 µs, sys: 1 µs, total: 62 µs\n",
      "Wall time: 67 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# and we can update this to use a line search along the gradient like this:\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.optimize import OptimizeResult\n",
    "\n",
    "import copy\n",
    "class LineSearchLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    # define custom line search for problem\n",
    "    @staticmethod\n",
    "    def line_search_function(eta,X,y,w,grad,C):\n",
    "        wnew = w + grad*eta\n",
    "        yhat = expit(X @ wnew)>0.5\n",
    "        return np.sum((y-yhat)**2) + C*np.sum(wnew**2)\n",
    "     \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            \n",
    "            # do line search in gradient direction, using scipy function\n",
    "            opts = {'maxiter':self.iters/20} # unclear exactly what this should be\n",
    "            res = minimize_scalar(self.line_search_function, # objective function to optimize\n",
    "                                  bounds=(self.eta/1000,self.eta*10), #bounds to optimize\n",
    "                                  args=(Xb,y,self.w_,gradient,self.C), # additional argument for objective function\n",
    "                                  method='bounded', # bounded optimization for speed\n",
    "                                  options=opts) # set max iterations\n",
    "            eta = res.x # get optimal learning rate\n",
    "            self.w_ += gradient*eta # set new function values\n",
    "                \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42 µs, sys: 1e+03 ns, total: 43 µs\n",
      "Wall time: 47 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if(self.reg == 0):\n",
    "            gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        elif(self.reg == 1):\n",
    "            gradient[1:] += np.sign(self.w_[1:]) * self.C\n",
    "        else:\n",
    "            gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "            gradient[1:] += np.sign(self.w_[1:]) * self.C\n",
    "        #gradient[1:] += 2 * self.w_[1:] * self.C\n",
    "        #gradient[1:] += np.sign(self.w_[1:]) * self.C\n",
    "        return gradient\n",
    "    \n",
    "    \n",
    "# slr = StochasticLogisticRegression(0.1,1000, C=0.001) # take a lot more steps!!\n",
    "\n",
    "# slr.fit(X,y)\n",
    "\n",
    "# yhat = slr.predict(X)\n",
    "# print(slr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59 µs, sys: 0 ns, total: 59 µs\n",
      "Wall time: 64.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# for this, we won't perform our own BFGS implementation \n",
    "# (it takes a good deal of code and understanding of the algorithm)\n",
    "# luckily for us, scipy has its own BFGS implementation:\n",
    "from scipy.optimize import fmin_bfgs\n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C,reg):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(w**2) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C,reg):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        if(reg == 0):\n",
    "            gradient[1:] += 2 * w[1:] * C\n",
    "        elif(reg == 1):\n",
    "            gradient[1:] += np.sign(w[1:]) * C\n",
    "        else:\n",
    "            gradient[1:] += 2 * w[1:] * C\n",
    "            gradient[1:] += np.sign(w[1:]) * C\n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C,self.reg), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        result = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C,self.reg), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False,\n",
    "                            retall=True)\n",
    "        self.iterations = len(result)\n",
    "        #print(\"Iterations: \", self.iterations)\n",
    "        #print(\"iterations: \", self.iterations)\n",
    "        self.w_ = self.w_.reshape((num_features,1))\n",
    "    def getIterations(self):\n",
    "        return self.iterations\n",
    "# bfgslr = BFGSBinaryLogisticRegression(_,2) # note that we need only a few iterations here\n",
    "\n",
    "# bfgslr.fit(X,y)\n",
    "# yhat = bfgslr.predict(X)\n",
    "# print(bfgslr)\n",
    "# print('Accuracy of: ',accuracy_score(y,yhat+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created the result (with parameter retall=True) variable to hold fmin_bfgs \n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_bfgs.html\n",
    "\n",
    "based on the description for  retall\n",
    "\n",
    "retall : bool, optional\n",
    "Return a list of results at each iteration if True.\n",
    "\n",
    "We return the length of the list for the iterations.\n",
    "\n",
    "EDIT: We realized that we are not getting iterations correctly and were not able to get iterations for our implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ability to choose between optimization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.0001, optimization=None,reg=0):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = []\n",
    "        self.optimization = optimization\n",
    "        self.reg = reg\n",
    "        self.params = {}\n",
    "\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            #hblr = HessianBinaryLogisticRegression(self.eta,self.iters,self.C)\n",
    "            if(self.optimization == \"BFGSBinaryLogisticRegression\"):\n",
    "                #self.iters = 10\n",
    "                hblr = BFGSBinaryLogisticRegression(self.eta,self.iters,self.C,self.reg)\n",
    "                #print(\"Iterations: \",hblr.getIterations())\n",
    "\n",
    "            elif(self.optimization == \"StochasticLogisticRegression\"):\n",
    "                #self.iters = 2000 #1000\n",
    "                hblr = StochasticLogisticRegression(self.eta,self.iters,self.C,self.reg)\n",
    "            else:\n",
    "                #self.iters = 100\n",
    "                #self.C = 0.001\n",
    "                hblr = LineSearchLogisticRegression(self.eta,self.iters,self.C,self.reg)\n",
    "\n",
    "            hblr.fit(X,y_binary)\n",
    "            #print(accuracy(y_binary,hblr.predict(X)))\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row\n",
    "    \n",
    "    def get_params(self,deep=False):\n",
    "        #return self.params\n",
    "        return dict(C=self.C,eta=self.eta,iterations=self.iters, optimization=self.optimization)\n",
    "\n",
    "    def set_params(self,**kwds):\n",
    "        print(kwds)\n",
    "        self.C = kwds['C']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Optimization Techniques, Etas, Iterations, and L1/L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "For  BFGSBinaryLogisticRegression  eta:  0.1 Iterations:  10 Regularization:  L1  : Accuracy of:  0.40099009901\n",
      "confusion matrix\n",
      " [[ 7 10  6  3  0]\n",
      " [ 4 11  3  1  4]\n",
      " [ 5 10 15 11  8]\n",
      " [ 1  4 12 18 15]\n",
      " [ 2  2  9 11 30]]\n",
      "====Iteration 1  ====\n",
      "For  BFGSBinaryLogisticRegression  eta:  0.1 Iterations:  10 Regularization:  L1  : Accuracy of:  0.40099009901\n",
      "confusion matrix\n",
      " [[14  5  7  2  0]\n",
      " [ 7  6  9  6  4]\n",
      " [ 7  5 22 11  7]\n",
      " [ 3  3 15 21 11]\n",
      " [ 0  1  7 11 18]]\n",
      "====Iteration 2  ====\n",
      "For  BFGSBinaryLogisticRegression  eta:  0.1 Iterations:  10 Regularization:  L1  : Accuracy of:  0.371287128713\n",
      "confusion matrix\n",
      " [[ 9 13  7  2  0]\n",
      " [ 7 13  5  9  3]\n",
      " [11  7 15  9  7]\n",
      " [ 2  0 11 18 14]\n",
      " [ 2  3  4 11 20]]\n",
      "====Iteration 0  ====\n",
      "For  StochasticLogisticRegression  eta:  0.1 Iterations:  5000 Regularization:  L2  : Accuracy of:  0.207920792079\n",
      "confusion matrix\n",
      " [[ 0  0  0  4 16]\n",
      " [ 0  0  0 11 20]\n",
      " [ 0  0  0 23 31]\n",
      " [ 0  0  0 31 19]\n",
      " [ 0  0  0 36 11]]\n",
      "====Iteration 1  ====\n",
      "For  StochasticLogisticRegression  eta:  0.1 Iterations:  5000 Regularization:  L2  : Accuracy of:  0.123762376238\n",
      "confusion matrix\n",
      " [[19  0  0  0  0]\n",
      " [38  0  2  0  0]\n",
      " [46  0  6  0  0]\n",
      " [44  0  3  0  0]\n",
      " [43  0  1  0  0]]\n",
      "====Iteration 2  ====\n",
      "For  StochasticLogisticRegression  eta:  0.1 Iterations:  5000 Regularization:  L2  : Accuracy of:  0.252475247525\n",
      "confusion matrix\n",
      " [[ 0  3  0 17  0]\n",
      " [ 0  2  0 37  0]\n",
      " [ 0  0  0 48  0]\n",
      " [ 0  0  0 49  0]\n",
      " [ 0  0  0 46  0]]\n",
      "====Iteration 0  ====\n",
      "For  LineSearchLogisticRegression  eta:  0.001 Iterations:  150 Regularization:  L1 and L2  : Accuracy of:  0.272277227723\n",
      "confusion matrix\n",
      " [[ 0  0 13 12  0]\n",
      " [ 0  0 11 26  0]\n",
      " [ 0  0 13 37  0]\n",
      " [ 0  0  4 42  0]\n",
      " [ 0  0  5 39  0]]\n",
      "====Iteration 1  ====\n",
      "For  LineSearchLogisticRegression  eta:  0.001 Iterations:  150 Regularization:  L1 and L2  : Accuracy of:  0.207920792079\n",
      "confusion matrix\n",
      " [[ 0  0  7 21  0]\n",
      " [ 0  0 17 25  0]\n",
      " [ 0  1 12 36  0]\n",
      " [ 0  0 13 30  0]\n",
      " [ 0  0  4 36  0]]\n",
      "====Iteration 2  ====\n",
      "For  LineSearchLogisticRegression  eta:  0.001 Iterations:  150 Regularization:  L1 and L2  : Accuracy of:  0.262376237624\n",
      "confusion matrix\n",
      " [[ 0  0  6 18  0]\n",
      " [ 0  0  9 29  0]\n",
      " [ 0  0  8 38  0]\n",
      " [ 0  0  3 45  0]\n",
      " [ 0  0  4 42  0]]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "\n",
    "from sklearn import metrics as mt\n",
    "with np.errstate(all='ignore'):\n",
    "    # first we create a reusable logisitic regression object\n",
    "    #   here we can setup the object with different learning parameters and constants\n",
    "\n",
    "    optimizations = [\"BFGSBinaryLogisticRegression\",\"StochasticLogisticRegression\",\"LineSearchLogisticRegression\"]\n",
    "    #optimizations = [\"BFGSBinaryLogisticRegression\",\"BFGSBinaryLogisticRegression\",\"BFGSBinaryLogisticRegression\"]\n",
    "    etas = [0.1, 0.1, 0.001]\n",
    "    iters = [10, 5000, 150]\n",
    "    regs = [0,1,2]\n",
    "\n",
    "    for optimization,eta,iter_,reg in zip(optimizations,etas,iters,regs):\n",
    "        lr_clf = MultiClassLogisticRegression(eta=eta,iterations=iter_, C=0.02, optimization=optimization,reg=reg) # get object\n",
    "\n",
    "\n",
    "        # now we can use the cv_object that we setup before to iterate through the \n",
    "        #    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "        #    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "        iter_num=0\n",
    "        # the indices are the rows used for training and testing in each iteration\n",
    "        for train_indices, test_indices in cv_object.split(X,y): \n",
    "            # I will create new variables here so that it is more obvious what \n",
    "            # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "            # but it makes this code less readable)\n",
    "            X_train = (X[train_indices])\n",
    "            y_train = y[train_indices]\n",
    "\n",
    "        #     print(X_train)\n",
    "        #     print(y_train)\n",
    "\n",
    "            X_test = (X[test_indices])\n",
    "            y_test = y[test_indices]\n",
    "\n",
    "    #         st = time.time()\n",
    "\n",
    "            lr_clf.fit(X_train,y_train)  # train object\n",
    "    #         t = (time.time() -st)\n",
    "    #         lr_clf_times.append(t)\n",
    "\n",
    "            lr_clf.fit(X_train,y_train)\n",
    "\n",
    "            # train the reusable logisitc regression model on the training data\n",
    "            y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "            # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "            acc = mt.accuracy_score(y_test,y_hat+1)\n",
    "    #         lr_clf_accuracies.append(acc)\n",
    "    #         cost_accuracies.append([acc])\n",
    "\n",
    "            conf = mt.confusion_matrix(y_test,y_hat+1)\n",
    "            print(\"====Iteration\",iter_num,\" ====\")\n",
    "            if(reg == 0):\n",
    "                label = \"L1\"\n",
    "            elif(reg == 1):\n",
    "                label = \"L2\"\n",
    "            else:\n",
    "                label = \"L1 and L2\"\n",
    "            print('For ',optimization,' eta: ',eta, \"Iterations: \",iter_,\"Regularization: \",label,' : Accuracy of: ',acc)\n",
    "\n",
    "            #print(\"accuracy\", acc )\n",
    "            print(\"confusion matrix\\n\",conf)\n",
    "            iter_num+=1\n",
    "\n",
    "        \n",
    "    # Also note that every time you run the above code\n",
    "    #   it randomly creates a new training and testing set, \n",
    "    #   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While varying between Steepest Descent, Stochastic Gradient Descent, and Newton’s Method, the regularization parameters L1, L2, and L1/L2, iterations from 10,150,5000, and etas 0.1 or 0.001, we have found our optimal combination is BFGS, an eta of 0.1, 10 iterations, and L1 Regularization. The combinations with higher accuracies have greater amount of non-zero numbers in their confusion matrices. As the values along the diagonals of the confusion matrices indicate accurate predictions, this makes sense. For Stochastic Logistic Regression, we have around two to three target classes with zeroes in columns which shows that there are no accurate predictions for that target class, perhaps due to fewer instances for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Pipelining PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "For  BFGSBinaryLogisticRegression  eta:  0.1 Iterations:  10 Regularization:  L1  : Accuracy of:  0.252475247525\n",
      "confusion matrix\n",
      " [[ 7  8 10  4  3]\n",
      " [12  8 16  5  6]\n",
      " [ 9  6 12  9  8]\n",
      " [ 2  3 13 10 17]\n",
      " [ 1  1  3 15 14]]\n",
      "Accurate predictions 51\n",
      "====Iteration 1  ====\n",
      "For  BFGSBinaryLogisticRegression  eta:  0.1 Iterations:  10 Regularization:  L1  : Accuracy of:  0.341584158416\n",
      "confusion matrix\n",
      " [[10  7  8  4  1]\n",
      " [ 6  7  9  7  2]\n",
      " [ 4  9 15  8 10]\n",
      " [ 2  5  9 14 16]\n",
      " [ 2  2  9 13 23]]\n",
      "Accurate predictions 69\n",
      "====Iteration 2  ====\n",
      "For  BFGSBinaryLogisticRegression  eta:  0.1 Iterations:  10 Regularization:  L1  : Accuracy of:  0.361386138614\n",
      "confusion matrix\n",
      " [[ 7  9 10  0  2]\n",
      " [ 7 12  9  4  0]\n",
      " [ 2  6 18  5 11]\n",
      " [ 5  8 17 13  9]\n",
      " [ 2  4  3 16 23]]\n",
      "Accurate predictions 73\n",
      "====Iteration 0  ====\n",
      "For  StochasticLogisticRegression  eta:  0.1 Iterations:  5000 Regularization:  L2  : Accuracy of:  0.356435643564\n",
      "confusion matrix\n",
      " [[18  2  3  1  1]\n",
      " [17  9  6  8  1]\n",
      " [ 7  6 13 12  4]\n",
      " [ 4 10 11 11 21]\n",
      " [ 0  4  5  7 21]]\n",
      "Accurate predictions 72\n",
      "====Iteration 1  ====\n",
      "For  StochasticLogisticRegression  eta:  0.1 Iterations:  5000 Regularization:  L2  : Accuracy of:  0.331683168317\n",
      "confusion matrix\n",
      " [[ 9  6  2  3  5]\n",
      " [11  8 10  5  3]\n",
      " [ 6  3 15  8  5]\n",
      " [ 6  6 19 13  8]\n",
      " [ 3  2 14 10 22]]\n",
      "Accurate predictions 67\n",
      "====Iteration 2  ====\n",
      "For  StochasticLogisticRegression  eta:  0.1 Iterations:  5000 Regularization:  L2  : Accuracy of:  0.29702970297\n",
      "confusion matrix\n",
      " [[ 7  9  9  6  1]\n",
      " [10 12  6  5  4]\n",
      " [ 6  7 14  7 10]\n",
      " [ 5 10 16  5 12]\n",
      " [ 2  5  4  8 22]]\n",
      "Accurate predictions 60\n",
      "====Iteration 0  ====\n",
      "For  LineSearchLogisticRegression  eta:  0.001 Iterations:  150 Regularization:  L1 and L2  : Accuracy of:  0.287128712871\n",
      "confusion matrix\n",
      " [[ 9  7  2  1  3]\n",
      " [ 7  8  9  5  5]\n",
      " [10 10  5  7 19]\n",
      " [10  6  4  8 24]\n",
      " [ 5  2  5  3 28]]\n",
      "Accurate predictions 58\n",
      "====Iteration 1  ====\n",
      "For  LineSearchLogisticRegression  eta:  0.001 Iterations:  150 Regularization:  L1 and L2  : Accuracy of:  0.321782178218\n",
      "confusion matrix\n",
      " [[13  9  0  1  2]\n",
      " [11  7  1  6  4]\n",
      " [ 9 12  7  5 14]\n",
      " [ 6 10  5  7 23]\n",
      " [ 5  6  1  7 31]]\n",
      "Accurate predictions 65\n",
      "====Iteration 2  ====\n",
      "For  LineSearchLogisticRegression  eta:  0.001 Iterations:  150 Regularization:  L1 and L2  : Accuracy of:  0.336633663366\n",
      "confusion matrix\n",
      " [[18  7  3  0  0]\n",
      " [14  8  4  4  3]\n",
      " [13 11  7 10 18]\n",
      " [ 6  2  3  8 24]\n",
      " [ 2  2  2  6 27]]\n",
      "Accurate predictions 68\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "\n",
    "optimizations = [\"BFGSBinaryLogisticRegression\",\"StochasticLogisticRegression\",\"LineSearchLogisticRegression\"]\n",
    "#optimizations = [\"BFGSBinaryLogisticRegression\",\"BFGSBinaryLogisticRegression\",\"BFGSBinaryLogisticRegression\"]\n",
    "etas = [0.1, 0.1, 0.001]\n",
    "iters = [10, 5000, 150]\n",
    "regs = [0,1,2]\n",
    "components = 90\n",
    "pca = PCA(n_components=components)\n",
    "\n",
    "with np.errstate(all='ignore'):\n",
    "    for optimization,eta,iter_,reg in zip(optimizations,etas,iters,regs):\n",
    "        mglr = MultiClassLogisticRegression(eta=eta,iterations=iter_, C=0.02, optimization=optimization,reg=reg) # get object\n",
    "        lr_clf = Pipeline([ ('pca', pca), (\"multiclasslogregression\", mglr)]) # get object\n",
    "\n",
    "\n",
    "        # now we can use the cv_object that we setup before to iterate through the \n",
    "        #    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "        #    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "        iter_num=0\n",
    "        # the indices are the rows used for training and testing in each iteration\n",
    "        for train_indices, test_indices in cv_object.split(X,y): \n",
    "            # I will create new variables here so that it is more obvious what \n",
    "            # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "            # but it makes this code less readable)\n",
    "            X_train = (X[train_indices])\n",
    "            y_train = y[train_indices]\n",
    "\n",
    "        #     print(X_train)\n",
    "        #     print(y_train)\n",
    "\n",
    "            X_test = (X[test_indices])\n",
    "            y_test = y[test_indices]\n",
    "\n",
    "    #         st = time.time()\n",
    "\n",
    "            lr_clf.fit(X_train,y_train)  # train object\n",
    "    #         t = (time.time() -st)\n",
    "    #         lr_clf_times.append(t)\n",
    "\n",
    "            lr_clf.fit(X_train,y_train)\n",
    "\n",
    "            # train the reusable logisitc regression model on the training data\n",
    "            y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "            # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "            acc = mt.accuracy_score(y_test,y_hat+1)\n",
    "    #         lr_clf_accuracies.append(acc)\n",
    "    #         cost_accuracies.append([acc])\n",
    "\n",
    "            conf = mt.confusion_matrix(y_test,y_hat+1)\n",
    "            print(\"====Iteration\",iter_num,\" ====\")\n",
    "            if(reg == 0):\n",
    "                label = \"L1\"\n",
    "            elif(reg == 1):\n",
    "                label = \"L2\"\n",
    "            else:\n",
    "                label = \"L1 and L2\"\n",
    "            print('For ',optimization,' eta: ',eta, \"Iterations: \",iter_,\"Regularization: \",label,' : Accuracy of: ',acc)\n",
    "\n",
    "            #print(\"accuracy\", acc )\n",
    "            print(\"confusion matrix\\n\",conf)\n",
    "            iter_num+=1\n",
    "            print(\"Accurate predictions\",np.sum(np.diagonal(conf)))\n",
    "\n",
    "\n",
    "        \n",
    "    # Also note that every time you run the above code\n",
    "    #   it randomly creates a new training and testing set, \n",
    "    #   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We included PCA to see if it would improve accuracy. BFGS did not see much improvement but LineSearch and Stochastic saw increases of ~ 10%. Our confusion matrices also increased in non zero values in the diagonals which means more accurate predictions were made. We noticed that before PCA, we had rows and columns of zeroes, which means no predictions were being made at all. These sets of matrices make much more sense. We decided to use the Pipeline object provided by sklearn to include PCA. We originally did use PCA and were recommended to try the Pipeline object. Otherwise, we would have used PCA from the start with our data. After trial and error, we decided to go with 90 components for PCA. We did not see improvement in accuracy when we used 70-110 components. We did see a significant decrease in accuracy when we choses components to be less than 40 (15-20%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best: eta: 0.1, reg = 0, iters = 10, acc = 35%\n",
    "eta: eta .001, iters 150, reg 2, acc 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting Values of C to Achieve Best Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.371287128713\n",
      "confusion matrix\n",
      " [[ 8  6  9  2  1]\n",
      " [ 8  9 14  6  1]\n",
      " [ 4  1 23 14  7]\n",
      " [ 3  7 11 13 14]\n",
      " [ 1  2  4 12 22]]\n",
      "Accurate predictions 75\n",
      "====Iteration 1  ====\n",
      "accuracy 0.361386138614\n",
      "confusion matrix\n",
      " [[ 6 18  3  0  2]\n",
      " [ 7 32  4  0  1]\n",
      " [ 1 37 14  2  3]\n",
      " [ 2 22  5  5  3]\n",
      " [ 1 14  1  3 16]]\n",
      "Accurate predictions 73\n",
      "====Iteration 2  ====\n",
      "accuracy 0.356435643564\n",
      "confusion matrix\n",
      " [[ 7  8  3  1  2]\n",
      " [10  9 11  3  3]\n",
      " [ 3 13 22 11  7]\n",
      " [ 3  4  9 11 17]\n",
      " [ 1  1  8 12 23]]\n",
      "Accurate predictions 72\n",
      "====Iteration 0  ====\n",
      "accuracy 0.361386138614\n",
      "confusion matrix\n",
      " [[ 8 11  3  2  1]\n",
      " [ 5  4 12 11  2]\n",
      " [ 3 11 19 16  4]\n",
      " [ 1  7 14 19 17]\n",
      " [ 0  2  3  4 23]]\n",
      "Accurate predictions 73\n",
      "====Iteration 1  ====\n",
      "accuracy 0.376237623762\n",
      "confusion matrix\n",
      " [[ 9 15  6  1  1]\n",
      " [ 6  7  7  6  3]\n",
      " [ 3 10 18 11  8]\n",
      " [ 2  4 14 11 15]\n",
      " [ 0  1  6  7 31]]\n",
      "Accurate predictions 76\n",
      "====Iteration 2  ====\n",
      "accuracy 0.391089108911\n",
      "confusion matrix\n",
      " [[ 8  8  9  3  1]\n",
      " [ 8  9  9  5  1]\n",
      " [ 3  8 18 17  6]\n",
      " [ 1  3  9 17 18]\n",
      " [ 2  1  4  7 27]]\n",
      "Accurate predictions 79\n",
      "====Iteration 0  ====\n",
      "accuracy 0.277227722772\n",
      "confusion matrix\n",
      " [[ 4 15  5  0  0]\n",
      " [ 4 25  6  1  0]\n",
      " [ 4 36  5  3  4]\n",
      " [ 1 40  3  3  8]\n",
      " [ 0 10  2  4 19]]\n",
      "Accurate predictions 56\n",
      "====Iteration 1  ====\n",
      "accuracy 0.420792079208\n",
      "confusion matrix\n",
      " [[ 8  7  9  5  2]\n",
      " [ 7  7 16  5  2]\n",
      " [ 3  6 26 11  7]\n",
      " [ 1  3  9 18  9]\n",
      " [ 0  0  4 11 26]]\n",
      "Accurate predictions 85\n",
      "====Iteration 2  ====\n",
      "accuracy 0.366336633663\n",
      "confusion matrix\n",
      " [[ 8  7  7  2  1]\n",
      " [ 7 11 10  9  1]\n",
      " [ 1 11 16  9  7]\n",
      " [ 5  6  8 19 14]\n",
      " [ 4  2 11  6 20]]\n",
      "Accurate predictions 74\n",
      "====Iteration 0  ====\n",
      "accuracy 0.311881188119\n",
      "confusion matrix\n",
      " [[ 8  9  7  2  0]\n",
      " [ 8  5 12  8  2]\n",
      " [ 2  9  9 14  6]\n",
      " [ 1  3 16 17 20]\n",
      " [ 1  0  9 10 24]]\n",
      "Accurate predictions 63\n",
      "====Iteration 1  ====\n",
      "accuracy 0.336633663366\n",
      "confusion matrix\n",
      " [[ 8  9  9  1  1]\n",
      " [ 7  9 12  6  1]\n",
      " [ 1  5 16 13  9]\n",
      " [ 0  8 18  9 18]\n",
      " [ 0  4  4  8 26]]\n",
      "Accurate predictions 68\n",
      "====Iteration 2  ====\n",
      "accuracy 0.361386138614\n",
      "confusion matrix\n",
      " [[ 6  7  5  4  1]\n",
      " [ 9  8 15  6  1]\n",
      " [ 1  4 19 19  9]\n",
      " [ 3  6 10 17 21]\n",
      " [ 0  0  3  5 23]]\n",
      "Accurate predictions 73\n",
      "====Iteration 0  ====\n",
      "accuracy 0.351485148515\n",
      "confusion matrix\n",
      " [[ 4  8  6  3  0]\n",
      " [10  8  7  5  2]\n",
      " [ 7  7 10 18  9]\n",
      " [ 2  5  8 20 14]\n",
      " [ 2  3  2 13 29]]\n",
      "Accurate predictions 71\n",
      "====Iteration 1  ====\n",
      "accuracy 0.39603960396\n",
      "confusion matrix\n",
      " [[ 7 12 10  2  0]\n",
      " [ 4  6  8  2  1]\n",
      " [ 5 11 16  8  6]\n",
      " [ 1  4 13 14 13]\n",
      " [ 4  2  5 11 37]]\n",
      "Accurate predictions 80\n",
      "====Iteration 2  ====\n",
      "accuracy 0.381188118812\n",
      "confusion matrix\n",
      " [[ 5  6 11  5  1]\n",
      " [ 5  7 12  7  2]\n",
      " [ 7  4 25  6  5]\n",
      " [ 2  4 23 14 10]\n",
      " [ 1  0  6  8 26]]\n",
      "Accurate predictions 77\n",
      "====Iteration 0  ====\n",
      "accuracy 0.311881188119\n",
      "confusion matrix\n",
      " [[ 3 10  8  8  3]\n",
      " [ 6  9 11  7  4]\n",
      " [ 4  7 19 17  7]\n",
      " [ 0  6 12  8 10]\n",
      " [ 0  3  6 10 24]]\n",
      "Accurate predictions 63\n",
      "====Iteration 1  ====\n",
      "accuracy 0.445544554455\n",
      "confusion matrix\n",
      " [[14  7  3  3  0]\n",
      " [10  6  6  7  1]\n",
      " [ 2 11 24 13  4]\n",
      " [ 1  3 17 20 12]\n",
      " [ 1  0  6  5 26]]\n",
      "Accurate predictions 90\n",
      "====Iteration 2  ====\n",
      "accuracy 0.391089108911\n",
      "confusion matrix\n",
      " [[ 5  6  6  1  1]\n",
      " [ 9  6 16  8  0]\n",
      " [ 4  8 19 12  8]\n",
      " [ 1  4 13 19 13]\n",
      " [ 0  0  5  8 30]]\n",
      "Accurate predictions 79\n",
      "====Iteration 0  ====\n",
      "accuracy 0.410891089109\n",
      "confusion matrix\n",
      " [[ 8  6  7  1  0]\n",
      " [ 4 11 10  3  2]\n",
      " [ 0 17 21 17  9]\n",
      " [ 3  7  6 18 12]\n",
      " [ 0  1  4 10 25]]\n",
      "Accurate predictions 83\n",
      "====Iteration 1  ====\n",
      "accuracy 0.405940594059\n",
      "confusion matrix\n",
      " [[ 9  9  9  0  0]\n",
      " [ 9  9 13  4  1]\n",
      " [ 1  7 20 11  3]\n",
      " [ 1  4 18 17 10]\n",
      " [ 1  1  6 12 27]]\n",
      "Accurate predictions 82\n",
      "====Iteration 2  ====\n",
      "accuracy 0.311881188119\n",
      "confusion matrix\n",
      " [[ 6  4  8  3  1]\n",
      " [12  7 10  6  3]\n",
      " [ 1  8 10 17 11]\n",
      " [ 2  5 12 15 17]\n",
      " [ 4  0  7  8 25]]\n",
      "Accurate predictions 63\n",
      "====Iteration 0  ====\n",
      "accuracy 0.29702970297\n",
      "confusion matrix\n",
      " [[ 8 16  5  0  1]\n",
      " [ 8 25  4  1  1]\n",
      " [ 2 34  4  7  4]\n",
      " [ 1 28  3  7  5]\n",
      " [ 2 16  1  3 16]]\n",
      "Accurate predictions 60\n",
      "====Iteration 1  ====\n",
      "accuracy 0.361386138614\n",
      "confusion matrix\n",
      " [[17 11  4  5  0]\n",
      " [10 12  6  3  2]\n",
      " [ 5 11  9 13  7]\n",
      " [ 0  8 13 13  7]\n",
      " [ 1  3  4 16 22]]\n",
      "Accurate predictions 73\n",
      "====Iteration 2  ====\n",
      "accuracy 0.420792079208\n",
      "confusion matrix\n",
      " [[ 7 10  5  1  0]\n",
      " [10 11 11  6  2]\n",
      " [ 3  5 23 12  3]\n",
      " [ 2  7 12 17 11]\n",
      " [ 1  2  6  8 27]]\n",
      "Accurate predictions 85\n",
      "====Iteration 0  ====\n",
      "accuracy 0.361386138614\n",
      "confusion matrix\n",
      " [[ 9  2  4  2  0]\n",
      " [ 7 10 14 11  2]\n",
      " [ 5  8 13 12  8]\n",
      " [ 0  5 18 16  7]\n",
      " [ 0  1 12 11 25]]\n",
      "Accurate predictions 73\n",
      "====Iteration 1  ====\n",
      "accuracy 0.455445544554\n",
      "confusion matrix\n",
      " [[ 8  7  4  3  0]\n",
      " [15 12 11  0  2]\n",
      " [ 4  5 19  8  4]\n",
      " [ 3  3 11 19 14]\n",
      " [ 4  0  4  8 34]]\n",
      "Accurate predictions 92\n",
      "====Iteration 2  ====\n",
      "accuracy 0.376237623762\n",
      "confusion matrix\n",
      " [[10  5  4  5  1]\n",
      " [10  9 15  8  1]\n",
      " [ 2  2 18 14  3]\n",
      " [ 2  0 17 12 22]\n",
      " [ 3  2  2  8 27]]\n",
      "Accurate predictions 76\n",
      "====Iteration 0  ====\n",
      "accuracy 0.381188118812\n",
      "confusion matrix\n",
      " [[ 9 10  4  2  1]\n",
      " [ 9 14  2  8  2]\n",
      " [ 5  5 14 11  3]\n",
      " [ 1  2 11 18 13]\n",
      " [ 2  3  7 24 22]]\n",
      "Accurate predictions 77\n",
      "====Iteration 1  ====\n",
      "accuracy 0.361386138614\n",
      "confusion matrix\n",
      " [[ 7  9 11  2  2]\n",
      " [ 5  7 17  5  0]\n",
      " [ 2  6 15 12  9]\n",
      " [ 3  1 12 19 13]\n",
      " [ 1  4  4 11 25]]\n",
      "Accurate predictions 73\n",
      "====Iteration 2  ====\n",
      "accuracy 0.366336633663\n",
      "confusion matrix\n",
      " [[ 9  6  7  3  0]\n",
      " [10  5 13  4  1]\n",
      " [ 3  8 22  8  4]\n",
      " [ 1  6 19 12 12]\n",
      " [ 0  1  4 18 26]]\n",
      "Accurate predictions 74\n",
      "====Iteration 0  ====\n",
      "accuracy 0.405940594059\n",
      "confusion matrix\n",
      " [[ 7  5  8  2  0]\n",
      " [ 6  8 10  5  1]\n",
      " [ 5  8 18 15  5]\n",
      " [ 2  4 10 23 14]\n",
      " [ 3  1  5 11 26]]\n",
      "Accurate predictions 82\n",
      "====Iteration 1  ====\n",
      "accuracy 0.381188118812\n",
      "confusion matrix\n",
      " [[ 6  9  5  3  1]\n",
      " [ 9  8  9  7  1]\n",
      " [ 4  6 15 12  8]\n",
      " [ 4  2 11 15 16]\n",
      " [ 0  2  7  9 33]]\n",
      "Accurate predictions 77\n",
      "====Iteration 2  ====\n",
      "accuracy 0.30198019802\n",
      "confusion matrix\n",
      " [[ 8  7 11  1  0]\n",
      " [ 4  7 16  6  1]\n",
      " [ 2  6 13 11  4]\n",
      " [ 1  3 23 12 12]\n",
      " [ 2  1  9 21 21]]\n",
      "Accurate predictions 61\n",
      "====Iteration 0  ====\n",
      "accuracy 0.376237623762\n",
      "confusion matrix\n",
      " [[10  8  9  2  1]\n",
      " [ 6  8 13  4  4]\n",
      " [ 2 11 25  8  4]\n",
      " [ 1  3 16 14 15]\n",
      " [ 1  1  3 14 19]]\n",
      "Accurate predictions 76\n",
      "====Iteration 1  ====\n",
      "accuracy 0.371287128713\n",
      "confusion matrix\n",
      " [[ 8 10  4  2  1]\n",
      " [14  9  6  2  2]\n",
      " [ 4 13 18  9  5]\n",
      " [ 2  3 11 11 19]\n",
      " [ 1  2  2 15 29]]\n",
      "Accurate predictions 75\n",
      "====Iteration 2  ====\n",
      "accuracy 0.366336633663\n",
      "confusion matrix\n",
      " [[ 8 10  8  4  1]\n",
      " [ 4 11 12  7  0]\n",
      " [ 4 11 15 12  4]\n",
      " [ 2  4  7 15 19]\n",
      " [ 0  0  3 16 25]]\n",
      "Accurate predictions 74\n",
      "====Iteration 0  ====\n",
      "accuracy 0.341584158416\n",
      "confusion matrix\n",
      " [[ 5  7  8  4  0]\n",
      " [10 10 12  4  3]\n",
      " [ 3  5 15 10  4]\n",
      " [ 3  7 15 20 17]\n",
      " [ 1  1 10  9 19]]\n",
      "Accurate predictions 69\n",
      "====Iteration 1  ====\n",
      "accuracy 0.405940594059\n",
      "confusion matrix\n",
      " [[ 5  7  7  4  1]\n",
      " [ 7 10  6 10  3]\n",
      " [ 6  6 18 13  3]\n",
      " [ 1  4  8 19 11]\n",
      " [ 2  0  6 15 30]]\n",
      "Accurate predictions 82\n",
      "====Iteration 2  ====\n",
      "accuracy 0.361386138614\n",
      "confusion matrix\n",
      " [[ 6 11  5  1  2]\n",
      " [ 4 10  6  4  2]\n",
      " [ 4 12 14 13  5]\n",
      " [ 2  5 13 21 16]\n",
      " [ 1  2 11 10 22]]\n",
      "Accurate predictions 73\n",
      "====Iteration 0  ====\n",
      "accuracy 0.415841584158\n",
      "confusion matrix\n",
      " [[14  8  5  2  1]\n",
      " [12  8 13  4  3]\n",
      " [ 4  4 22 12  4]\n",
      " [ 1  3 12 16 17]\n",
      " [ 0  1  8  4 24]]\n",
      "Accurate predictions 84\n",
      "====Iteration 1  ====\n",
      "accuracy 0.381188118812\n",
      "confusion matrix\n",
      " [[ 8  7  8  2  2]\n",
      " [11  9 12  2  1]\n",
      " [ 5  5 19 18  4]\n",
      " [ 4  4 11  9 12]\n",
      " [ 0  0  5 12 32]]\n",
      "Accurate predictions 77\n",
      "====Iteration 2  ====\n",
      "accuracy 0.40099009901\n",
      "confusion matrix\n",
      " [[12  4 13  0  3]\n",
      " [11  9  9  7  0]\n",
      " [ 1 14 20  8  2]\n",
      " [ 2  3 12 15 18]\n",
      " [ 0  2  5  7 25]]\n",
      "Accurate predictions 81\n",
      "====Iteration 0  ====\n",
      "accuracy 0.336633663366\n",
      "confusion matrix\n",
      " [[13  8  7  1  1]\n",
      " [12  5 15  8  2]\n",
      " [ 9  7 14  8  7]\n",
      " [ 1  9 13 11  9]\n",
      " [ 2  0  5 10 25]]\n",
      "Accurate predictions 68\n",
      "====Iteration 1  ====\n",
      "accuracy 0.376237623762\n",
      "confusion matrix\n",
      " [[ 9  8  9  1  1]\n",
      " [ 7 11 12  6  2]\n",
      " [ 3  5 19  6  6]\n",
      " [ 2  8 18 14 18]\n",
      " [ 1  2  2  9 23]]\n",
      "Accurate predictions 76\n",
      "====Iteration 2  ====\n",
      "accuracy 0.40099009901\n",
      "confusion matrix\n",
      " [[ 9  4  4  1  0]\n",
      " [ 8  9 11  8  3]\n",
      " [ 2 13 21 14  5]\n",
      " [ 2  1 11 16 21]\n",
      " [ 1  0  4  8 26]]\n",
      "Accurate predictions 81\n",
      "====Iteration 0  ====\n",
      "accuracy 0.331683168317\n",
      "confusion matrix\n",
      " [[10 11  6  0  2]\n",
      " [ 9 11 13  7  3]\n",
      " [ 7  9  9  6  7]\n",
      " [ 1  3 12 11 23]\n",
      " [ 1  3  1 11 26]]\n",
      "Accurate predictions 67\n",
      "====Iteration 1  ====\n",
      "accuracy 0.386138613861\n",
      "confusion matrix\n",
      " [[13  9  5  4  1]\n",
      " [ 9 12 11  5  4]\n",
      " [ 2 14 18 17  4]\n",
      " [ 1  1  6 17 20]\n",
      " [ 0  2  3  6 18]]\n",
      "Accurate predictions 78\n",
      "====Iteration 2  ====\n",
      "accuracy 0.311881188119\n",
      "confusion matrix\n",
      " [[ 7  7  9  4  1]\n",
      " [ 6  8 18  7  3]\n",
      " [ 1 11 18  8  8]\n",
      " [ 2  3 13  7 21]\n",
      " [ 0  3  5  9 23]]\n",
      "Accurate predictions 63\n",
      "====Iteration 0  ====\n",
      "accuracy 0.326732673267\n",
      "confusion matrix\n",
      " [[ 8  4  9  0  2]\n",
      " [ 6 11 10  9  2]\n",
      " [ 5 10 18  9  6]\n",
      " [ 1  8 12  9 20]\n",
      " [ 1  3 12  7 20]]\n",
      "Accurate predictions 66\n",
      "====Iteration 1  ====\n",
      "accuracy 0.376237623762\n",
      "confusion matrix\n",
      " [[ 5 11  8  2  1]\n",
      " [ 7 12  9  6  3]\n",
      " [ 3 13 16 10  7]\n",
      " [ 2  2 15 14 17]\n",
      " [ 1  0  3  6 29]]\n",
      "Accurate predictions 76\n",
      "====Iteration 2  ====\n",
      "accuracy 0.425742574257\n",
      "confusion matrix\n",
      " [[ 9  9  5  1  3]\n",
      " [ 6 13 14  3  1]\n",
      " [ 4  5 18 15  4]\n",
      " [ 0  2 12 20 15]\n",
      " [ 0  1  4 12 26]]\n",
      "Accurate predictions 86\n",
      "====Iteration 0  ====\n",
      "accuracy 0.381188118812\n",
      "confusion matrix\n",
      " [[ 7  6 10  6  2]\n",
      " [ 5 10 13  8  2]\n",
      " [ 6  4 16  6  4]\n",
      " [ 2  1 18 16 17]\n",
      " [ 3  1  6  5 28]]\n",
      "Accurate predictions 77\n",
      "====Iteration 1  ====\n",
      "accuracy 0.351485148515\n",
      "confusion matrix\n",
      " [[10  4  7  6  0]\n",
      " [ 7  7  9 11  1]\n",
      " [ 2 11 12 11  6]\n",
      " [ 0 12 13 12 10]\n",
      " [ 1  1  6 13 30]]\n",
      "Accurate predictions 71\n",
      "====Iteration 2  ====\n",
      "accuracy 0.386138613861\n",
      "confusion matrix\n",
      " [[10  8  4  1  1]\n",
      " [ 5 15 11  8  2]\n",
      " [ 3  7 18 14  7]\n",
      " [ 5  5  5 15 15]\n",
      " [ 0  2  6 15 20]]\n",
      "Accurate predictions 78\n",
      "====Iteration 0  ====\n",
      "accuracy 0.376237623762\n",
      "confusion matrix\n",
      " [[ 5  6  6  1  1]\n",
      " [ 9  9 13  7  0]\n",
      " [ 6  7 17 11  6]\n",
      " [ 3  4 13 14 20]\n",
      " [ 2  1  4  6 31]]\n",
      "Accurate predictions 76\n",
      "====Iteration 1  ====\n",
      "accuracy 0.381188118812\n",
      "confusion matrix\n",
      " [[10 11  9  4  2]\n",
      " [12  9 14  6  2]\n",
      " [ 7  4 20  7  6]\n",
      " [ 1  4  7 16  9]\n",
      " [ 1  1  7 11 22]]\n",
      "Accurate predictions 77\n",
      "====Iteration 2  ====\n",
      "accuracy 0.386138613861\n",
      "confusion matrix\n",
      " [[10  8  3  0  2]\n",
      " [ 6  3 10  3  1]\n",
      " [ 3  6 20  8 10]\n",
      " [ 1  7 18 16 15]\n",
      " [ 1  0  5 17 29]]\n",
      "Accurate predictions 78\n",
      "====Iteration 0  ====\n",
      "accuracy 0.306930693069\n",
      "confusion matrix\n",
      " [[ 6 14  7  2  1]\n",
      " [ 6 10  8  2  4]\n",
      " [ 7 10 11 13  9]\n",
      " [ 1  4 11 12 23]\n",
      " [ 1  0  4 13 23]]\n",
      "Accurate predictions 62\n",
      "====Iteration 1  ====\n",
      "accuracy 0.425742574257\n",
      "confusion matrix\n",
      " [[10  8  1  2  2]\n",
      " [10 11  8  5  2]\n",
      " [ 7  7 14 11  6]\n",
      " [ 1  1 11 16 15]\n",
      " [ 1  0  2 16 35]]\n",
      "Accurate predictions 86\n",
      "====Iteration 2  ====\n",
      "accuracy 0.346534653465\n",
      "confusion matrix\n",
      " [[ 6  3  7  2  1]\n",
      " [12  7 12  7  3]\n",
      " [ 7  5 18  8  9]\n",
      " [ 2  4 13 13 18]\n",
      " [ 3  0 10  6 26]]\n",
      "Accurate predictions 70\n",
      "====Iteration 0  ====\n",
      "accuracy 0.341584158416\n",
      "confusion matrix\n",
      " [[ 9  4 11  1  0]\n",
      " [14  8  8  3  7]\n",
      " [ 7  3 14 13 11]\n",
      " [ 4  3  9 14 19]\n",
      " [ 2  1  3 10 24]]\n",
      "Accurate predictions 69\n",
      "====Iteration 1  ====\n",
      "accuracy 0.29702970297\n",
      "confusion matrix\n",
      " [[ 9  9  7  3  0]\n",
      " [ 9  9 13  5  4]\n",
      " [ 9  5 12 18 13]\n",
      " [ 1  1 19  8 14]\n",
      " [ 1  1  4  6 22]]\n",
      "Accurate predictions 60\n",
      "====Iteration 2  ====\n",
      "accuracy 0.366336633663\n",
      "confusion matrix\n",
      " [[ 7  7  3  3  1]\n",
      " [ 9  9 12  4  5]\n",
      " [10  8 16  9 11]\n",
      " [ 3  3  8 11 19]\n",
      " [ 2  0  4  7 31]]\n",
      "Accurate predictions 74\n",
      "====Iteration 0  ====\n",
      "accuracy 0.267326732673\n",
      "confusion matrix\n",
      " [[ 4  8  6  3  1]\n",
      " [ 4 10 11  2  3]\n",
      " [ 6  8 13  5 12]\n",
      " [ 3 10 16  9 13]\n",
      " [ 3  2 14 18 18]]\n",
      "Accurate predictions 54\n",
      "====Iteration 1  ====\n",
      "accuracy 0.341584158416\n",
      "confusion matrix\n",
      " [[ 8 10 10  2  1]\n",
      " [ 8 15 10  3  2]\n",
      " [ 4  9 17  7  7]\n",
      " [ 4  1 15  8 21]\n",
      " [ 2  0  9  8 21]]\n",
      "Accurate predictions 69\n",
      "====Iteration 2  ====\n",
      "accuracy 0.376237623762\n",
      "confusion matrix\n",
      " [[10 11  4  5  3]\n",
      " [ 8 13 11  4  5]\n",
      " [ 5  4 19  7  5]\n",
      " [ 0  4 15 12 18]\n",
      " [ 1  1  3 12 22]]\n",
      "Accurate predictions 76\n",
      "====Iteration 0  ====\n",
      "accuracy 0.356435643564\n",
      "confusion matrix\n",
      " [[ 9  8 13  4  1]\n",
      " [ 9  8 14  4  5]\n",
      " [ 3  6 15  7  7]\n",
      " [ 2  4 12 11 17]\n",
      " [ 3  2  4  5 29]]\n",
      "Accurate predictions 72\n",
      "====Iteration 1  ====\n",
      "accuracy 0.366336633663\n",
      "confusion matrix\n",
      " [[ 8  7  5  3  1]\n",
      " [ 8  8  3  6  5]\n",
      " [ 3 10 15 14 11]\n",
      " [ 2  6 10  9 22]\n",
      " [ 1  2  2  7 34]]\n",
      "Accurate predictions 74\n",
      "====Iteration 2  ====\n",
      "accuracy 0.326732673267\n",
      "confusion matrix\n",
      " [[ 6  8  9  0  4]\n",
      " [11 10 11  4  3]\n",
      " [ 4  4 16  6 11]\n",
      " [ 4  5 17 12 19]\n",
      " [ 2  1  7  6 22]]\n",
      "Accurate predictions 66\n",
      "====Iteration 0  ====\n",
      "accuracy 0.341584158416\n",
      "confusion matrix\n",
      " [[ 8  8  3  3  3]\n",
      " [ 8  6  7  7  7]\n",
      " [ 5  3 17 11 11]\n",
      " [ 0  4 17 13 20]\n",
      " [ 5  3  2  6 25]]\n",
      "Accurate predictions 69\n",
      "====Iteration 1  ====\n",
      "accuracy 0.356435643564\n",
      "confusion matrix\n",
      " [[ 9  9  5  3  0]\n",
      " [ 7 11  7  6  2]\n",
      " [ 3 12 19  9 15]\n",
      " [ 4  2 14 11 18]\n",
      " [ 1  1  4  8 22]]\n",
      "Accurate predictions 72\n",
      "====Iteration 2  ====\n",
      "accuracy 0.30198019802\n",
      "confusion matrix\n",
      " [[ 8  8  3  4  2]\n",
      " [ 7  9  4  7  3]\n",
      " [ 8 12 16 15  7]\n",
      " [ 4  7 10  7 19]\n",
      " [ 3  0  8 10 21]]\n",
      "Accurate predictions 61\n",
      "====Iteration 0  ====\n",
      "accuracy 0.351485148515\n",
      "confusion matrix\n",
      " [[10  7  7  1  0]\n",
      " [ 9  8 13  5  5]\n",
      " [ 5  6 14 10  9]\n",
      " [ 3  6  7  5 24]\n",
      " [ 2  1  5  6 34]]\n",
      "Accurate predictions 71\n",
      "====Iteration 1  ====\n",
      "accuracy 0.336633663366\n",
      "confusion matrix\n",
      " [[ 5  2 10  3  3]\n",
      " [ 3  6 15  6  5]\n",
      " [ 7  5 18  9 14]\n",
      " [ 3  5  8  9 18]\n",
      " [ 4  2  5  7 30]]\n",
      "Accurate predictions 68\n",
      "====Iteration 2  ====\n",
      "accuracy 0.346534653465\n",
      "confusion matrix\n",
      " [[10  4  2  1  3]\n",
      " [ 9  5  9  2  3]\n",
      " [ 5  9 16 10 14]\n",
      " [ 6  5 13  8 24]\n",
      " [ 2  2  4  5 31]]\n",
      "Accurate predictions 70\n",
      "====Iteration 0  ====\n",
      "accuracy 0.346534653465\n",
      "confusion matrix\n",
      " [[ 6  6 10  3  4]\n",
      " [ 8  9  9  3  4]\n",
      " [ 4 13 13  4 13]\n",
      " [ 4  8  6 12 18]\n",
      " [ 2  2  4  7 30]]\n",
      "Accurate predictions 70\n",
      "====Iteration 1  ====\n",
      "accuracy 0.267326732673\n",
      "confusion matrix\n",
      " [[ 8  6  7  2  3]\n",
      " [12  2 12  5  2]\n",
      " [ 3  4 11  8 14]\n",
      " [ 2  4 19  7 26]\n",
      " [ 5  1  6  7 26]]\n",
      "Accurate predictions 54\n",
      "====Iteration 2  ====\n",
      "accuracy 0.346534653465\n",
      "confusion matrix\n",
      " [[10  4  5  3  2]\n",
      " [ 8  8 14  3  4]\n",
      " [ 6  8 11 11 11]\n",
      " [ 3  5 13  9 21]\n",
      " [ 3  1  3  4 32]]\n",
      "Accurate predictions 70\n",
      "====Iteration 0  ====\n",
      "accuracy 0.316831683168\n",
      "confusion matrix\n",
      " [[ 9  1  9  0  4]\n",
      " [10  5 11  5  8]\n",
      " [ 6  9 15  6 22]\n",
      " [ 4  2 10  8 20]\n",
      " [ 1  1  4  5 27]]\n",
      "Accurate predictions 64\n",
      "====Iteration 1  ====\n",
      "accuracy 0.30198019802\n",
      "confusion matrix\n",
      " [[11  5  8  1  3]\n",
      " [12  8  5  7  6]\n",
      " [ 7  8 11  9 12]\n",
      " [ 4  4  7  5 23]\n",
      " [ 4  4  5  7 26]]\n",
      "Accurate predictions 61\n",
      "====Iteration 2  ====\n",
      "accuracy 0.292079207921\n",
      "confusion matrix\n",
      " [[ 5  7  7  7  4]\n",
      " [14  5 10  4  7]\n",
      " [ 6  4 14  4 15]\n",
      " [ 2  4 14  4 25]\n",
      " [ 2  1  3  3 31]]\n",
      "Accurate predictions 59\n",
      "====Iteration 0  ====\n",
      "accuracy 0.371287128713\n",
      "confusion matrix\n",
      " [[ 9  8  6  1  5]\n",
      " [ 4  9  9  4  9]\n",
      " [ 1  6 20  8 15]\n",
      " [ 3  5  9  9 22]\n",
      " [ 2  1  5  4 28]]\n",
      "Accurate predictions 75\n",
      "====Iteration 1  ====\n",
      "accuracy 0.326732673267\n",
      "confusion matrix\n",
      " [[19  3  2  4  2]\n",
      " [16  4  5  6  4]\n",
      " [13  6  8 10 11]\n",
      " [14  2  2  8 26]\n",
      " [ 3  1  0  6 27]]\n",
      "Accurate predictions 66\n",
      "====Iteration 2  ====\n",
      "accuracy 0.331683168317\n",
      "confusion matrix\n",
      " [[ 5 13  2  1  1]\n",
      " [14  9  3  7  4]\n",
      " [14 10  8 15  5]\n",
      " [ 2  6  9 10 12]\n",
      " [ 3  3  2  9 35]]\n",
      "Accurate predictions 67\n",
      "====Iteration 0  ====\n",
      "accuracy 0.361386138614\n",
      "confusion matrix\n",
      " [[13  9  2  2  3]\n",
      " [ 9  7  6  0 10]\n",
      " [ 6 11 10  2 16]\n",
      " [ 5  8  5  8 23]\n",
      " [ 3  3  2  4 35]]\n",
      "Accurate predictions 73\n",
      "====Iteration 1  ====\n",
      "accuracy 0.356435643564\n",
      "confusion matrix\n",
      " [[ 8  7  5  3  0]\n",
      " [11 12  7  1  7]\n",
      " [13  9 12  5 12]\n",
      " [ 6  4  9 10 12]\n",
      " [ 4  4  5  6 30]]\n",
      "Accurate predictions 72\n",
      "====Iteration 2  ====\n",
      "accuracy 0.351485148515\n",
      "confusion matrix\n",
      " [[18  4  3  1  1]\n",
      " [17  8  4  7  7]\n",
      " [ 9  8  7  4 19]\n",
      " [ 3  5  5 10 19]\n",
      " [ 4  1  2  8 28]]\n",
      "Accurate predictions 71\n",
      "====Iteration 0  ====\n",
      "accuracy 0.376237623762\n",
      "confusion matrix\n",
      " [[14  5  3  0  1]\n",
      " [11  8  6  0  9]\n",
      " [10  7 10  6 18]\n",
      " [ 3  6 12 10 20]\n",
      " [ 3  0  1  5 34]]\n",
      "Accurate predictions 76\n",
      "====Iteration 1  ====\n",
      "accuracy 0.391089108911\n",
      "confusion matrix\n",
      " [[13  3  3  1  2]\n",
      " [12 10  5  1  8]\n",
      " [15  4  5  1 13]\n",
      " [ 8  4  8 14 25]\n",
      " [ 5  2  0  3 37]]\n",
      "Accurate predictions 79\n",
      "====Iteration 2  ====\n",
      "accuracy 0.356435643564\n",
      "confusion matrix\n",
      " [[14 14  2  1  1]\n",
      " [12  5  5  3  3]\n",
      " [ 9  9 12  3 13]\n",
      " [ 5  6  5  6 23]\n",
      " [ 2  2  1 11 35]]\n",
      "Accurate predictions 72\n",
      "====Iteration 0  ====\n",
      "accuracy 0.316831683168\n",
      "confusion matrix\n",
      " [[12  3  2  5  5]\n",
      " [15  9  2  8  7]\n",
      " [ 9  5  8 10 11]\n",
      " [ 7  2  7 11 19]\n",
      " [ 1  4  2 14 24]]\n",
      "Accurate predictions 64\n",
      "====Iteration 1  ====\n",
      "accuracy 0.366336633663\n",
      "confusion matrix\n",
      " [[19  2  4  1  3]\n",
      " [14  9  6  3  2]\n",
      " [12  9  5  4  8]\n",
      " [10  0 12 10 17]\n",
      " [ 2  0  6 13 31]]\n",
      "Accurate predictions 74\n",
      "====Iteration 2  ====\n",
      "accuracy 0.316831683168\n",
      "confusion matrix\n",
      " [[13  8  3  0  2]\n",
      " [15  8  6  6  7]\n",
      " [ 7 10 11  4 22]\n",
      " [ 7  4  7  4 19]\n",
      " [ 1  2  2  6 28]]\n",
      "Accurate predictions 64\n",
      "====Iteration 0  ====\n",
      "accuracy 0.356435643564\n",
      "confusion matrix\n",
      " [[14  8  3  1  5]\n",
      " [12 10  6  2  4]\n",
      " [10  9 10  5 16]\n",
      " [ 4  6  4  5 23]\n",
      " [ 4  2  4  2 33]]\n",
      "Accurate predictions 72\n",
      "====Iteration 1  ====\n",
      "accuracy 0.321782178218\n",
      "confusion matrix\n",
      " [[10  1  2  1  3]\n",
      " [13  8  7  3 10]\n",
      " [15  7  5  4 19]\n",
      " [ 9  6  9 10 17]\n",
      " [ 0  1  2  8 32]]\n",
      "Accurate predictions 65\n",
      "====Iteration 2  ====\n",
      "accuracy 0.306930693069\n",
      "confusion matrix\n",
      " [[11  6  5  2  3]\n",
      " [18  9  7  1  5]\n",
      " [14  5  6  6 15]\n",
      " [ 9  3  5  6 24]\n",
      " [ 2  2  3  5 30]]\n",
      "Accurate predictions 62\n",
      "====Iteration 0  ====\n",
      "accuracy 0.282178217822\n",
      "confusion matrix\n",
      " [[13  6  3  3  2]\n",
      " [11  8  5  7  7]\n",
      " [12  8  7  8  9]\n",
      " [ 7  6  7  5 34]\n",
      " [ 0  2  1  7 24]]\n",
      "Accurate predictions 57\n",
      "====Iteration 1  ====\n",
      "accuracy 0.341584158416\n",
      "confusion matrix\n",
      " [[14  3  6  3  3]\n",
      " [17 10  4  3  5]\n",
      " [11  7  7  6 12]\n",
      " [ 5  7 14  8 15]\n",
      " [ 4  2  2  4 30]]\n",
      "Accurate predictions 69\n",
      "====Iteration 2  ====\n",
      "accuracy 0.351485148515\n",
      "confusion matrix\n",
      " [[20  5  2  5  4]\n",
      " [23  8  2  3  2]\n",
      " [10  6  6  9 13]\n",
      " [ 8  5  0  8 19]\n",
      " [ 6  2  1  6 29]]\n",
      "Accurate predictions 71\n",
      "====Iteration 0  ====\n",
      "accuracy 0.371287128713\n",
      "confusion matrix\n",
      " [[10 11  2  1  0]\n",
      " [17 10  1  2  3]\n",
      " [10  4  3 13 14]\n",
      " [13  6  1  9 16]\n",
      " [ 3  4  0  6 43]]\n",
      "Accurate predictions 75\n",
      "====Iteration 1  ====\n",
      "accuracy 0.262376237624\n",
      "confusion matrix\n",
      " [[19  4  2  1  1]\n",
      " [20  4  7  5  5]\n",
      " [18  8  6 10 11]\n",
      " [10  3  5  8 22]\n",
      " [ 1  2  5  9 16]]\n",
      "Accurate predictions 53\n",
      "====Iteration 2  ====\n",
      "accuracy 0.381188118812\n",
      "confusion matrix\n",
      " [[ 7 11  5  2  3]\n",
      " [ 7 18  3  5  5]\n",
      " [ 6 11  5  8 11]\n",
      " [ 9  3  4  7 18]\n",
      " [ 1  5  3  5 40]]\n",
      "Accurate predictions 77\n",
      "====Iteration 0  ====\n",
      "accuracy 0.292079207921\n",
      "confusion matrix\n",
      " [[ 3 17  2  1  3]\n",
      " [ 7 15  1  1  2]\n",
      " [ 8 20  4  8 20]\n",
      " [ 8  7  7  8 16]\n",
      " [ 1  4  3  7 29]]\n",
      "Accurate predictions 59\n",
      "====Iteration 1  ====\n",
      "accuracy 0.292079207921\n",
      "confusion matrix\n",
      " [[ 9 14  2  2  4]\n",
      " [15  9  1  7  3]\n",
      " [11  7  7  6 10]\n",
      " [ 8  7  5 21 19]\n",
      " [ 2  1  4 15 13]]\n",
      "Accurate predictions 59\n",
      "====Iteration 2  ====\n",
      "accuracy 0.217821782178\n",
      "confusion matrix\n",
      " [[ 1 22  0  2  5]\n",
      " [ 6 14  1  3  6]\n",
      " [14 16  2  8 11]\n",
      " [12  8  0 14 16]\n",
      " [ 6  3  0 19 13]]\n",
      "Accurate predictions 44\n",
      "====Iteration 0  ====\n",
      "accuracy 0.356435643564\n",
      "confusion matrix\n",
      " [[10  8  3  0  3]\n",
      " [13 11  7  0  6]\n",
      " [10  9 13  0 19]\n",
      " [ 6  4 15  0 18]\n",
      " [ 3  1  5  0 38]]\n",
      "Accurate predictions 72\n",
      "====Iteration 1  ====\n",
      "accuracy 0.29702970297\n",
      "confusion matrix\n",
      " [[ 3 19  3  0  4]\n",
      " [ 7 18  3  4  5]\n",
      " [13 13  8  2 15]\n",
      " [ 6  6  5  6 26]\n",
      " [ 0  4  5  2 25]]\n",
      "Accurate predictions 60\n",
      "====Iteration 2  ====\n",
      "accuracy 0.30198019802\n",
      "confusion matrix\n",
      " [[ 0 10  9  1  7]\n",
      " [ 2 13  7  0  7]\n",
      " [ 0 13  8  2 23]\n",
      " [ 0 15  9  1 20]\n",
      " [ 0 10  5  1 39]]\n",
      "Accurate predictions 61\n",
      "====Iteration 0  ====\n",
      "accuracy 0.19801980198\n",
      "confusion matrix\n",
      " [[ 5  1  9  0  6]\n",
      " [14  2 10  1 16]\n",
      " [16  0  9  1 22]\n",
      " [ 8  2  8  0 34]\n",
      " [ 7  3  4  0 24]]\n",
      "Accurate predictions 40\n",
      "====Iteration 1  ====\n",
      "accuracy 0.277227722772\n",
      "confusion matrix\n",
      " [[17  1  0  1 10]\n",
      " [12  0  1  1  8]\n",
      " [19  3  2  2 24]\n",
      " [14  0  2  2 34]\n",
      " [11  1  1  1 35]]\n",
      "Accurate predictions 56\n",
      "====Iteration 2  ====\n",
      "accuracy 0.232673267327\n",
      "confusion matrix\n",
      " [[10  2  2  1 10]\n",
      " [22  1  0  2 14]\n",
      " [14  0  4  0 21]\n",
      " [14  1  9  1 29]\n",
      " [ 7  2  5  0 31]]\n",
      "Accurate predictions 47\n",
      "====Iteration 0  ====\n",
      "accuracy 0.242574257426\n",
      "confusion matrix\n",
      " [[17  0  0 12  1]\n",
      " [16  0  2 13  2]\n",
      " [19  0  4 25  5]\n",
      " [ 9  1  4 21  4]\n",
      " [10  0  3 27  7]]\n",
      "Accurate predictions 49\n",
      "====Iteration 1  ====\n",
      "accuracy 0.19801980198\n",
      "confusion matrix\n",
      " [[ 6  0  5  3 15]\n",
      " [10  0  6  3 11]\n",
      " [11  0  6 17 12]\n",
      " [10  0  9 11 18]\n",
      " [ 5  0  9 18 17]]\n",
      "Accurate predictions 40\n",
      "====Iteration 2  ====\n",
      "accuracy 0.20297029703\n",
      "confusion matrix\n",
      " [[ 9  1  0  3  6]\n",
      " [23  2  0  3 10]\n",
      " [20  5  0  7 24]\n",
      " [14  1  1  3 25]\n",
      " [ 9  1  3  5 27]]\n",
      "Accurate predictions 41\n",
      "====Iteration 0  ====\n",
      "accuracy 0.257425742574\n",
      "confusion matrix\n",
      " [[ 5 11  0  1 13]\n",
      " [ 6 13  1  0 11]\n",
      " [ 5  9  1  1 29]\n",
      " [ 5 12  2  1 30]\n",
      " [ 2  8  4  0 32]]\n",
      "Accurate predictions 52\n",
      "====Iteration 1  ====\n",
      "accuracy 0.242574257426\n",
      "confusion matrix\n",
      " [[11  3  0  0  6]\n",
      " [17  3  1  1 20]\n",
      " [12 12  0  0 22]\n",
      " [ 9 13  1  0 25]\n",
      " [ 3  6  2  0 35]]\n",
      "Accurate predictions 49\n",
      "====Iteration 2  ====\n",
      "accuracy 0.282178217822\n",
      "confusion matrix\n",
      " [[ 9  5  3  0 14]\n",
      " [ 5  9  4  1 21]\n",
      " [ 4 10  4  1 23]\n",
      " [ 3 11  1  0 25]\n",
      " [ 2  9  3  0 35]]\n",
      "Accurate predictions 57\n",
      "====Iteration 0  ====\n",
      "accuracy 0.232673267327\n",
      "confusion matrix\n",
      " [[17  0  0  1 13]\n",
      " [24  0  0  2  9]\n",
      " [21  1  0  0 23]\n",
      " [18  2  1  1 24]\n",
      " [12  4  0  0 29]]\n",
      "Accurate predictions 47\n",
      "====Iteration 1  ====\n",
      "accuracy 0.30198019802\n",
      "confusion matrix\n",
      " [[17  1  1  1 17]\n",
      " [14  1  0  4 10]\n",
      " [14  3  0  4 18]\n",
      " [12  5  4  5 18]\n",
      " [10  3  1  1 38]]\n",
      "Accurate predictions 61\n",
      "====Iteration 2  ====\n",
      "accuracy 0.257425742574\n",
      "confusion matrix\n",
      " [[12  2  0  7  6]\n",
      " [18  2  0  4  9]\n",
      " [10  8  1 16  5]\n",
      " [11  5  1 20 14]\n",
      " [ 8  8  3 15 17]]\n",
      "Accurate predictions 52\n",
      "====Iteration 0  ====\n",
      "accuracy 0.267326732673\n",
      "confusion matrix\n",
      " [[ 3 18  0  5  8]\n",
      " [ 4 18  0  4  8]\n",
      " [ 3 19  2  4 26]\n",
      " [ 1 13  1  2 22]\n",
      " [ 0 10  2  0 29]]\n",
      "Accurate predictions 54\n",
      "====Iteration 1  ====\n",
      "accuracy 0.193069306931\n",
      "confusion matrix\n",
      " [[ 8  1  0  1 14]\n",
      " [23  2  0  2  9]\n",
      " [18  1  0  1 29]\n",
      " [17  4  1  1 29]\n",
      " [12  0  1  0 28]]\n",
      "Accurate predictions 39\n",
      "====Iteration 2  ====\n",
      "accuracy 0.242574257426\n",
      "confusion matrix\n",
      " [[18  1  1  8  6]\n",
      " [18  2  0 10  0]\n",
      " [17  1  1 15 14]\n",
      " [15  3  5 15 15]\n",
      " [ 5  2  2 15 13]]\n",
      "Accurate predictions 49\n",
      "====Iteration 0  ====\n",
      "accuracy 0.227722772277\n",
      "confusion matrix\n",
      " [[10  0  0  1 12]\n",
      " [16  0  4  0 16]\n",
      " [ 9  0  4  1 22]\n",
      " [12  0  6  1 37]\n",
      " [12  0  6  2 31]]\n",
      "Accurate predictions 46\n",
      "====Iteration 1  ====\n",
      "accuracy 0.242574257426\n",
      "confusion matrix\n",
      " [[11  1  1  0 14]\n",
      " [20  0  4  0 19]\n",
      " [15  0 11  2 22]\n",
      " [12  0  7  1 28]\n",
      " [ 6  1  1  0 26]]\n",
      "Accurate predictions 49\n",
      "====Iteration 2  ====\n",
      "accuracy 0.306930693069\n",
      "confusion matrix\n",
      " [[ 6 10  0  0 10]\n",
      " [ 7 17  1  1 12]\n",
      " [ 9 17  1  1 22]\n",
      " [ 4 12  0  3 25]\n",
      " [ 1  6  2  0 35]]\n",
      "Accurate predictions 62\n",
      "====Iteration 0  ====\n",
      "accuracy 0.247524752475\n",
      "confusion matrix\n",
      " [[17  0  0  0 11]\n",
      " [17  0  1  0 10]\n",
      " [31  1  0  0 33]\n",
      " [15  1  2  0 22]\n",
      " [ 6  1  1  0 33]]\n",
      "Accurate predictions 50\n",
      "====Iteration 1  ====\n",
      "accuracy 0.282178217822\n",
      "confusion matrix\n",
      " [[ 0  7  8  2  9]\n",
      " [ 3  9 12  1 10]\n",
      " [ 2  9 10  1 24]\n",
      " [ 0 11  7  0 28]\n",
      " [ 0  7  4  0 38]]\n",
      "Accurate predictions 57\n",
      "====Iteration 2  ====\n",
      "accuracy 0.19801980198\n",
      "confusion matrix\n",
      " [[10  0  0  0 11]\n",
      " [20  0  0  0 16]\n",
      " [20  0  1  0 29]\n",
      " [23  1  3  1 28]\n",
      " [ 8  2  1  0 28]]\n",
      "Accurate predictions 40\n",
      "====Iteration 0  ====\n",
      "accuracy 0.282178217822\n",
      "confusion matrix\n",
      " [[16  0  0  0 11]\n",
      " [17  1  0  0 13]\n",
      " [18  0  7  1 26]\n",
      " [ 9  1  9  1 25]\n",
      " [13  0  2  0 32]]\n",
      "Accurate predictions 57\n",
      "====Iteration 1  ====\n",
      "accuracy 0.272277227723\n",
      "confusion matrix\n",
      " [[ 5  0  9  1  9]\n",
      " [10  1  6  0 12]\n",
      " [12  0 22  1 27]\n",
      " [ 9  0 13  1 27]\n",
      " [ 6  1  4  0 26]]\n",
      "Accurate predictions 55\n",
      "====Iteration 2  ====\n",
      "accuracy 0.267326732673\n",
      "confusion matrix\n",
      " [[21  1  0  3  4]\n",
      " [25  1  1  8  9]\n",
      " [23  0  0 11 17]\n",
      " [11  1  5  5 19]\n",
      " [ 5  1  4  0 27]]\n",
      "Accurate predictions 54\n",
      "====Iteration 0  ====\n",
      "accuracy 0.237623762376\n",
      "confusion matrix\n",
      " [[17  1  0  1 14]\n",
      " [ 9  3  0  2 15]\n",
      " [20  3  0  2 33]\n",
      " [13  4  1  2 24]\n",
      " [ 4  5  1  2 26]]\n",
      "Accurate predictions 48\n",
      "====Iteration 1  ====\n",
      "accuracy 0.232673267327\n",
      "confusion matrix\n",
      " [[10  1  1  1  9]\n",
      " [10  2  4  4 10]\n",
      " [16  3  5  5 28]\n",
      " [ 8  1  6  0 36]\n",
      " [ 2  1  8  1 30]]\n",
      "Accurate predictions 47\n",
      "====Iteration 2  ====\n",
      "accuracy 0.207920792079\n",
      "confusion matrix\n",
      " [[13  0  1  2  8]\n",
      " [19  0  1  1 14]\n",
      " [21  0  1  4 19]\n",
      " [16  0  3  4 34]\n",
      " [12  1  2  2 24]]\n",
      "Accurate predictions 42\n",
      "====Iteration 0  ====\n",
      "accuracy 0.262376237624\n",
      "confusion matrix\n",
      " [[17  0  1  4  4]\n",
      " [18  2  5  4 10]\n",
      " [17  1  9 22  5]\n",
      " [ 8  2  6 14 18]\n",
      " [ 6  2  1 15 11]]\n",
      "Accurate predictions 53\n",
      "====Iteration 1  ====\n",
      "accuracy 0.207920792079\n",
      "confusion matrix\n",
      " [[ 6  1  0  2 12]\n",
      " [20  0  0  2  7]\n",
      " [25  0  1  2 27]\n",
      " [16  0  0  1 26]\n",
      " [18  0  2  0 34]]\n",
      "Accurate predictions 42\n",
      "====Iteration 2  ====\n",
      "accuracy 0.237623762376\n",
      "confusion matrix\n",
      " [[ 4  5  0  0 14]\n",
      " [10  7  0  0  5]\n",
      " [10 12  1  3 20]\n",
      " [ 6 14  2  4 31]\n",
      " [ 9 10  3  0 32]]\n",
      "Accurate predictions 48\n",
      "====Iteration 0  ====\n",
      "accuracy 0.217821782178\n",
      "confusion matrix\n",
      " [[14  0  0  2 11]\n",
      " [23  0  1  1  7]\n",
      " [23  1  0  2 19]\n",
      " [15  0  1  3 32]\n",
      " [12  1  3  4 27]]\n",
      "Accurate predictions 44\n",
      "====Iteration 1  ====\n",
      "accuracy 0.242574257426\n",
      "confusion matrix\n",
      " [[ 8  5  0  2 12]\n",
      " [12 12  0  0 17]\n",
      " [ 5 17  0  0 29]\n",
      " [ 5 11  0  0 24]\n",
      " [ 5  8  1  0 29]]\n",
      "Accurate predictions 49\n",
      "====Iteration 2  ====\n",
      "accuracy 0.193069306931\n",
      "confusion matrix\n",
      " [[10  1  0  0  9]\n",
      " [29  0  0  1 17]\n",
      " [23  0  0  2 29]\n",
      " [22  0  2  0 20]\n",
      " [ 7  1  0  0 29]]\n",
      "Accurate predictions 39\n",
      "====Iteration 0  ====\n",
      "accuracy 0.257425742574\n",
      "confusion matrix\n",
      " [[15  0  2  0 13]\n",
      " [15  1  1  0 15]\n",
      " [12  0  6  1 30]\n",
      " [ 7  0  8  2 29]\n",
      " [ 7  0  7  3 28]]\n",
      "Accurate predictions 52\n",
      "====Iteration 1  ====\n",
      "accuracy 0.247524752475\n",
      "confusion matrix\n",
      " [[15  3  0  1  7]\n",
      " [17  2  1  2 10]\n",
      " [21  3  0  5 26]\n",
      " [16  0  3  1 22]\n",
      " [12  2  1  0 32]]\n",
      "Accurate predictions 50\n",
      "====Iteration 2  ====\n",
      "accuracy 0.183168316832\n",
      "confusion matrix\n",
      " [[10  0  0  0  7]\n",
      " [22  2  1  0 12]\n",
      " [23  4  0  1 25]\n",
      " [17  8  2  0 28]\n",
      " [ 8  4  3  0 25]]\n",
      "Accurate predictions 37\n",
      "====Iteration 0  ====\n",
      "accuracy 0.237623762376\n",
      "confusion matrix\n",
      " [[12  1  3  6  5]\n",
      " [15  0  5 11  4]\n",
      " [18  0  7 11 12]\n",
      " [10  0  9 11 11]\n",
      " [ 8  2 10 13 18]]\n",
      "Accurate predictions 48\n",
      "====Iteration 1  ====\n",
      "accuracy 0.247524752475\n",
      "confusion matrix\n",
      " [[14  0  0  6  8]\n",
      " [22  1  0  5 12]\n",
      " [22  2  0  6 15]\n",
      " [17  1  2  8 15]\n",
      " [11  1  4  3 27]]\n",
      "Accurate predictions 50\n",
      "====Iteration 2  ====\n",
      "accuracy 0.20297029703\n",
      "confusion matrix\n",
      " [[13  1  0  1 10]\n",
      " [18  1  1  2 11]\n",
      " [21  2  1  4 24]\n",
      " [15  2  3  3 29]\n",
      " [12  1  3  1 23]]\n",
      "Accurate predictions 41\n",
      "====Iteration 0  ====\n",
      "accuracy 0.252475247525\n",
      "confusion matrix\n",
      " [[20  0  0  0  9]\n",
      " [18  0  0  3 14]\n",
      " [26  2  0  2 18]\n",
      " [15  1  0  0 28]\n",
      " [11  2  0  2 31]]\n",
      "Accurate predictions 51\n",
      "====Iteration 1  ====\n",
      "accuracy 0.242574257426\n",
      "confusion matrix\n",
      " [[16  0  2  3  6]\n",
      " [16  0  0  6  6]\n",
      " [18  2  4  5 19]\n",
      " [15  1  5  2 27]\n",
      " [11  2  2  7 27]]\n",
      "Accurate predictions 49\n",
      "====Iteration 2  ====\n",
      "accuracy 0.222772277228\n",
      "confusion matrix\n",
      " [[11  2  0  1 17]\n",
      " [15  0  1  1 13]\n",
      " [23  0  0  1 25]\n",
      " [19  2  0  2 27]\n",
      " [ 9  1  0  0 32]]\n",
      "Accurate predictions 45\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf_accuracies = []\n",
    "lr_clf_times = []\n",
    "lr_clf_mem = []\n",
    "\n",
    "costs = np.logspace(-3,1)\n",
    "costs.sort()\n",
    "\n",
    "cost_accuracies = []\n",
    "with np.errstate(all='ignore'):\n",
    "    for cost in costs:\n",
    "        mglr = MultiClassLogisticRegression(eta=0.1,iterations=10, C=cost, optimization=\"BFGSBinaryLogisticRegression\",reg=0) # get object\n",
    "\n",
    "        lr_clf = Pipeline([ ('pca', pca), (\"multiclasslogregression\", mglr)])\n",
    "        # now we can use the cv_object that we setup before to iterate through the \n",
    "        #    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "        #    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "        iter_num=0\n",
    "        # the indices are the rows used for training and testing in each iteration\n",
    "        for train_indices, test_indices in cv_object.split(X,y): \n",
    "            # I will create new variables here so that it is more obvious what \n",
    "            # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "            # but it makes this code less readable)\n",
    "            X_train = (X[train_indices])\n",
    "            y_train = y[train_indices]\n",
    "\n",
    "        #     print(X_train)\n",
    "        #     print(y_train)\n",
    "\n",
    "            X_test = (X[test_indices])\n",
    "            y_test = y[test_indices]\n",
    "\n",
    "            st = time.time()\n",
    "\n",
    "            mem = memory_usage((lr_clf.fit,(X_train,y_train))) # train object\n",
    "            t = (time.time() -st)\n",
    "            lr_clf_times.append(t)\n",
    "            lr_clf_mem.append(mem[0])\n",
    "\n",
    "            # train the reusable logisitc regression model on the training data\n",
    "            y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "            # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "            acc = mt.accuracy_score(y_test,y_hat+1)\n",
    "            lr_clf_accuracies.append(acc)\n",
    "            cost_accuracies.append([acc])\n",
    "\n",
    "            conf = mt.confusion_matrix(y_test,y_hat+1)\n",
    "            print(\"====Iteration\",iter_num,\" ====\")\n",
    "            print(\"accuracy\", acc )\n",
    "            print(\"confusion matrix\\n\",conf)\n",
    "            iter_num+=1\n",
    "            print(\"Accurate predictions\",np.sum(np.diagonal(conf)))\n",
    "\n",
    "\n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validity of Parameter Selections\n",
    "\n",
    "\n",
    "Using values other than 0.02 for BFGS Cost show some increases in our accuracy. We went from mid to high 30s values for accuracy to mid to low 40s. A small, but welcomed improvement. We do not see sums of the diagonals (correct predictions) to truly increase.\n",
    "\n",
    "We are \"data snooping.\" We have exhaustively checked for different values of Cost to get the most accuracy in our predictions. We have ran our code numerous times and have increased the chance that we received a good cost value by chance. We are trying to find correlations with young people wanting to shop at a large shopping center and by increasing our accuracy this way, we may draw wrong conclusions and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "c2 = [x for pair in zip(costs,costs,costs) for x in pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f8b0b70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEjCAYAAAAhczZxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110a19c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f8b0b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "li = [np.arange(1,151)]\n",
    "p = plt.plot(cost_accuracies)\n",
    "plt.title(\"Determining Optimal Value of Regularization Term C\")\n",
    "plt.xlabel('Value of C ')\n",
    "plt.ylabel('Accuracy (%) ')\n",
    "costs_plot = np.around(c2,decimals=2)\n",
    "plt.xticks(li[0],costs_plot, rotation=90)\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We were having trouble displaying the c values along the x axis so we displayed them below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001, 0.001, 0.001, 0.0012067926406393288, 0.0012067926406393288, 0.0012067926406393288, 0.0014563484775012444, 0.0014563484775012444, 0.0014563484775012444, 0.0017575106248547913, 0.0017575106248547913, 0.0017575106248547913, 0.0021209508879201904, 0.0021209508879201904, 0.0021209508879201904, 0.0025595479226995358, 0.0025595479226995358, 0.0025595479226995358, 0.0030888435964774815, 0.0030888435964774815, 0.0030888435964774815, 0.0037275937203149379, 0.0037275937203149379, 0.0037275937203149379, 0.0044984326689694442, 0.0044984326689694442, 0.0044984326689694442, 0.0054286754393238594, 0.0054286754393238594, 0.0054286754393238594, 0.0065512855685955088, 0.0065512855685955088, 0.0065512855685955088, 0.0079060432109077008, 0.0079060432109077008, 0.0079060432109077008, 0.0095409547634999446, 0.0095409547634999446, 0.0095409547634999446, 0.011513953993264469, 0.011513953993264469, 0.011513953993264469, 0.013894954943731374, 0.013894954943731374, 0.013894954943731374, 0.016768329368110076, 0.016768329368110076, 0.016768329368110076, 0.020235896477251564, 0.020235896477251564, 0.020235896477251564, 0.024420530945486511, 0.024420530945486511, 0.024420530945486511, 0.029470517025518096, 0.029470517025518096, 0.029470517025518096, 0.035564803062231282, 0.035564803062231282, 0.035564803062231282, 0.042919342601287762, 0.042919342601287762, 0.042919342601287762, 0.0517947467923121, 0.0517947467923121, 0.0517947467923121, 0.0625055192527397, 0.0625055192527397, 0.0625055192527397, 0.07543120063354615, 0.07543120063354615, 0.07543120063354615, 0.091029817799152174, 0.091029817799152174, 0.091029817799152174, 0.10985411419875583, 0.10985411419875583, 0.10985411419875583, 0.13257113655901082, 0.13257113655901082, 0.13257113655901082, 0.15998587196060574, 0.15998587196060574, 0.15998587196060574, 0.19306977288832497, 0.19306977288832497, 0.19306977288832497, 0.23299518105153719, 0.23299518105153719, 0.23299518105153719, 0.28117686979742279, 0.28117686979742279, 0.28117686979742279, 0.33932217718953261, 0.33932217718953261, 0.33932217718953261, 0.40949150623804231, 0.40949150623804231, 0.40949150623804231, 0.49417133613238334, 0.49417133613238334, 0.49417133613238334, 0.59636233165946428, 0.59636233165946428, 0.59636233165946428, 0.71968567300115138, 0.71968567300115138, 0.71968567300115138, 0.86851137375135201, 0.86851137375135201, 0.86851137375135201, 1.0481131341546852, 1.0481131341546852, 1.0481131341546852, 1.2648552168552958, 1.2648552168552958, 1.2648552168552958, 1.5264179671752318, 1.5264179671752318, 1.5264179671752318, 1.8420699693267144, 1.8420699693267144, 1.8420699693267144, 2.2229964825261934, 2.2229964825261934, 2.2229964825261934, 2.6826957952797246, 2.6826957952797246, 2.6826957952797246, 3.2374575428176433, 3.2374575428176433, 3.2374575428176433, 3.906939937054613, 3.906939937054613, 3.906939937054613, 4.7148663634573893, 4.7148663634573893, 4.7148663634573893, 5.689866029018293, 5.689866029018293, 5.689866029018293, 6.8664884500429979, 6.8664884500429979, 6.8664884500429979, 8.2864277285468422, 8.2864277285468422, 8.2864277285468422, 10.0, 10.0, 10.0]\n"
     ]
    }
   ],
   "source": [
    "print(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00449843266897\n"
     ]
    }
   ],
   "source": [
    "max_y = max(p[0].get_ydata())\n",
    "d = {}\n",
    "for pair in zip(c2,p[0].get_ydata()):\n",
    "    d[pair[1]] = pair[0]\n",
    "max_x = d[max_y]\n",
    "print(max_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further optimize the classifier, we plotted the sample values of the regularization parameter (0.001 - 10) against their respective accuracies and determined that there are usually two to three x regularization parameter values that would produce the highest accuracies. We found the c that produced the highest accuracy in the function above and use it to capture the performance of our \"best\" logistic regression optimization procedure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing our Best Logistic Regression Optimization Procedure to that of Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.376237623762\n",
      "confusion matrix\n",
      " [[ 8  8  6  2  1]\n",
      " [ 9  8  9  5  0]\n",
      " [ 7 11 20 10  9]\n",
      " [ 0  7 10 14 18]\n",
      " [ 2  2  4  6 26]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.351485148515\n",
      "confusion matrix\n",
      " [[ 7 12  7  3  0]\n",
      " [ 9 10 11  6  3]\n",
      " [ 3  6 19 15  6]\n",
      " [ 2  8 10  9 11]\n",
      " [ 1  4  1 13 26]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.415841584158\n",
      "confusion matrix\n",
      " [[ 7  4  7  1  4]\n",
      " [ 9 11 10  5  2]\n",
      " [ 6  6 19  9  3]\n",
      " [ 1  5 12 17 17]\n",
      " [ 1  3  5  8 30]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr_clf_accuracies = []\n",
    "lr_clf_times = []\n",
    "lr_clf_mem = []\n",
    "lr_clf_iterations = []\n",
    "\n",
    "\n",
    "\n",
    "mglr = MultiClassLogisticRegression(eta=0.1,iterations=10, C=max_x, optimization=\"BFGSBinaryLogisticRegression\",reg=0) # get object\n",
    "\n",
    "lr_clf = Pipeline([ ('pca', pca), (\"multiclasslogregression\", mglr)])\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "with np.errstate(all='ignore'):\n",
    "    for train_indices, test_indices in cv_object.split(X,y): \n",
    "        # I will create new variables here so that it is more obvious what \n",
    "        # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "        # but it makes this code less readable)\n",
    "        X_train = (X[train_indices])\n",
    "        y_train = y[train_indices]\n",
    "\n",
    "    #     print(X_train)\n",
    "    #     print(y_train)\n",
    "\n",
    "        X_test = (X[test_indices])\n",
    "        y_test = y[test_indices]\n",
    "\n",
    "        st = time.time()\n",
    "\n",
    "        mem = memory_usage((lr_clf.fit,(X_train,y_train))) # train object\n",
    "        t = (time.time() -st)\n",
    "        lr_clf_times.append(t)\n",
    "        lr_clf_mem.append(mem[0])\n",
    "\n",
    "        # train the reusable logisitc regression model on the training data\n",
    "        y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "        # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "        acc = mt.accuracy_score(y_test,y_hat+1)\n",
    "        lr_clf_accuracies.append(acc)\n",
    "        cost_accuracies.append([acc])\n",
    "\n",
    "        conf = mt.confusion_matrix(y_test,y_hat+1)\n",
    "        print(\"====Iteration\",iter_num,\" ====\")\n",
    "        print(\"accuracy\", acc )\n",
    "        print(\"confusion matrix\\n\",conf)\n",
    "        iter_num+=1\n",
    "for x in range(0,15):\n",
    "    lr_clf_iterations.append(2) \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing various optimizations we determined our \"best\" logistic regression procedure would be BFGS with an eta of 0.1, 10 Iterations, L1 Regularization, and a C regularization parameter of 0.02 based off the highest accuracy in our plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations  [ 69  22  33 173  71]\n",
      "====Iteration 0  ====\n",
      "accuracy 0.163366336634\n",
      "confusion matrix\n",
      " [[ 0  0  0  0  0  0]\n",
      " [ 2  5  7  5 10  0]\n",
      " [ 5  8  7  5 13  0]\n",
      " [ 6  4  9 11 13  0]\n",
      " [ 4  6 14  9 10  0]\n",
      " [ 7  6 16 10 10  0]]\n",
      "Iterations  [138  49  55  88 178]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.178217821782\n",
      "confusion matrix\n",
      " [[ 0  0  0  0  0  0]\n",
      " [ 5  2  8  4  7  0]\n",
      " [ 4  5  8  7  6  0]\n",
      " [ 5  8 13 14 12  0]\n",
      " [ 5  5 13  6 12  0]\n",
      " [ 5  9 11  9 19  0]]\n",
      "Iterations  [212 101  47  87 191]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.178217821782\n",
      "confusion matrix\n",
      " [[ 0  0  0  0  0  0]\n",
      " [ 6  4  4  4  6  0]\n",
      " [ 5  6 11  5 14  0]\n",
      " [ 4  6 12 11 11  0]\n",
      " [ 5  6 12  9 10  0]\n",
      " [ 4  7 14 11 15  0]]\n",
      "[0.36100220680236816, 0.28028416633605957, 0.3296501636505127]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "\n",
    "\n",
    "lr_sk = SKLogisticRegression(solver='lbfgs',class_weight='balanced',max_iter=500,C=0.002) \n",
    "\n",
    "lr_sk_accuracies = []\n",
    "lr_sk_times = []\n",
    "lr_sk_mem = []\n",
    "lr_sk_iterations = []\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "#     print(X_train)\n",
    "#     print(y_train)\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    st = time.time()\n",
    "    mem = memory_usage((lr_sk.fit,(X_train,y_train)))\n",
    "    #lr_sk.fit(X_train,y_train)\n",
    "    t = (time.time() -st)\n",
    "    lr_sk_times.append(t)\n",
    "    lr_sk_mem.append(mem[0])\n",
    "    #print(np.hstack((lr_sk.intercept_[:,np.newaxis],lr_sk.coef_)))\n",
    "    yhat = lr_sk.predict(X_test)\n",
    "    print(\"Iterations \",lr_sk.n_iter_)\n",
    "    lr_sk_iterations.append(lr_sk.n_iter_)\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    lr_sk_accuracies.append(acc)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "\n",
    "print(lr_sk_times)\n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, our scikit learn logistic regression optimization procedure has a row of zeros and a column of zeroes which we believe is most likely causing its low accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Performance Differences in terms of Accuracy, Training Time, Training Iterations, and Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498.4525203704834\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wHNaZbXx675AAAAAUlEQVSeP4/DqwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110a07e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110a07860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot([lr_sk_accuracies,lr_clf_accuracies])\n",
    "plt.title(\"Comparing Accuracies\")\n",
    "plt.xlabel('Implementation of Logistic Regression')\n",
    "plt.ylabel('Accuracy Percentage ')\n",
    "plt.xticks([1,2],['SKL','OURS'])\n",
    "plt.figure()\n",
    "print((time.time() -st)*100)\n",
    "# ax = fig.add_subplot(111)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have higher accuracy in our implementation but this is expected as we did not perform PCA on the SKL implementation and SKL confusion matrices had columns and rows of 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1147cb240>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1147cb9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1147cb240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot([lr_sk_times,lr_clf_times])\n",
    "plt.title(\"Comparing Training Times\")\n",
    "plt.xlabel('Implementation of Logistic Regression')\n",
    "plt.ylabel('Training Time (seconds) ')\n",
    "plt.xticks([1,2],['SKL','OURS'])\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best logistic regression optimization procedure's training time is 1.6 times greater than that of scikit learn's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11481dda0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEjMzK8VtEUO2AAAAC0lEQVSJxMzMSvn/2Zl+OEqJBUcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114839588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11481dda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot([lr_sk_mem,lr_clf_mem])\n",
    "plt.title(\"Comparing Memory \")\n",
    "plt.xlabel('Implementation of Logistic Regression')\n",
    "plt.ylabel('Memory Usage (mb) ')\n",
    "plt.xticks([1,2],['SKL','OURS'])\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best logistic regression optimization procedure's strength is its memory usage. Its training memory usage is considerably less than that of scikit learn's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1147e3278>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVsoJwszMSv0frqaj+6V8OnsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1147e3710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1147e3278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot([lr_sk_iterations,lr_clf_iterations])\n",
    "plt.title(\"Comparing Number of Iterations \")\n",
    "plt.xlabel('Implementation of Logistic Regression')\n",
    "plt.ylabel('Number of Iterations')\n",
    "plt.xticks([1,2],['SKL','OURS'])\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was difficult to compare our implementation's number of iterations to that of scikit learn's because ours was consistently returning two, which is most likely incorrect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing which implementation of Logistic Regression would be best for our case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we have higher accuracy than sklearn's implementation of BFGS, we acknowledge that we may not have implemented it properly. Our implementation has flaws (such as not being able to return iterations) and has not been tested like sklearn's. Our implementation was slower than sklearn's. We realize we did not use PCA on sklearn's implementation. Once we went back and used the Pipeline object, we saw that it created errors in trying to track memory and iterations. This likely will greatly affect the accuracies between our implementations.\n",
    "\n",
    "If we could improve our implementation we would recommend it (as it does use scipy) but since we know it has flaws and could not directly compare our implementations (due to PCA and Pipelining), we would recommend using sklearn's implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Below is our usage of our implementation of GridSearchCV. We chose a scoring for accuracy and used our BFGS implementation as our estimator. We want to find the best Cost value that GridSearchCV recommends and compare it with our decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.001}\n",
      "{'C': 0.001}\n",
      "{'C': 0.001}\n",
      "{'C': 0.0012067926406393288}\n",
      "{'C': 0.0012067926406393288}\n",
      "{'C': 0.0012067926406393288}\n",
      "{'C': 0.0014563484775012444}\n",
      "{'C': 0.0014563484775012444}\n",
      "{'C': 0.0014563484775012444}\n",
      "{'C': 0.0017575106248547913}\n",
      "{'C': 0.0017575106248547913}\n",
      "{'C': 0.0017575106248547913}\n",
      "{'C': 0.0021209508879201904}\n",
      "{'C': 0.0021209508879201904}\n",
      "{'C': 0.0021209508879201904}\n",
      "{'C': 0.0025595479226995358}\n",
      "{'C': 0.0025595479226995358}\n",
      "{'C': 0.0025595479226995358}\n",
      "{'C': 0.0030888435964774815}\n",
      "{'C': 0.0030888435964774815}\n",
      "{'C': 0.0030888435964774815}\n",
      "{'C': 0.0037275937203149379}\n",
      "{'C': 0.0037275937203149379}\n",
      "{'C': 0.0037275937203149379}\n",
      "{'C': 0.0044984326689694442}\n",
      "{'C': 0.0044984326689694442}\n",
      "{'C': 0.0044984326689694442}\n",
      "{'C': 0.0054286754393238594}\n",
      "{'C': 0.0054286754393238594}\n",
      "{'C': 0.0054286754393238594}\n",
      "{'C': 0.0065512855685955088}\n",
      "{'C': 0.0065512855685955088}\n",
      "{'C': 0.0065512855685955088}\n",
      "{'C': 0.0079060432109077008}\n",
      "{'C': 0.0079060432109077008}\n",
      "{'C': 0.0079060432109077008}\n",
      "{'C': 0.0095409547634999446}\n",
      "{'C': 0.0095409547634999446}\n",
      "{'C': 0.0095409547634999446}\n",
      "{'C': 0.011513953993264469}\n",
      "{'C': 0.011513953993264469}\n",
      "{'C': 0.011513953993264469}\n",
      "{'C': 0.013894954943731374}\n",
      "{'C': 0.013894954943731374}\n",
      "{'C': 0.013894954943731374}\n",
      "{'C': 0.016768329368110076}\n",
      "{'C': 0.016768329368110076}\n",
      "{'C': 0.016768329368110076}\n",
      "{'C': 0.020235896477251564}\n",
      "{'C': 0.020235896477251564}\n",
      "{'C': 0.020235896477251564}\n",
      "{'C': 0.024420530945486511}\n",
      "{'C': 0.024420530945486511}\n",
      "{'C': 0.024420530945486511}\n",
      "{'C': 0.029470517025518096}\n",
      "{'C': 0.029470517025518096}\n",
      "{'C': 0.029470517025518096}\n",
      "{'C': 0.035564803062231282}\n",
      "{'C': 0.035564803062231282}\n",
      "{'C': 0.035564803062231282}\n",
      "{'C': 0.042919342601287762}\n",
      "{'C': 0.042919342601287762}\n",
      "{'C': 0.042919342601287762}\n",
      "{'C': 0.0517947467923121}\n",
      "{'C': 0.0517947467923121}\n",
      "{'C': 0.0517947467923121}\n",
      "{'C': 0.0625055192527397}\n",
      "{'C': 0.0625055192527397}\n",
      "{'C': 0.0625055192527397}\n",
      "{'C': 0.07543120063354615}\n",
      "{'C': 0.07543120063354615}\n",
      "{'C': 0.07543120063354615}\n",
      "{'C': 0.091029817799152174}\n",
      "{'C': 0.091029817799152174}\n",
      "{'C': 0.091029817799152174}\n",
      "{'C': 0.10985411419875583}\n",
      "{'C': 0.10985411419875583}\n",
      "{'C': 0.10985411419875583}\n",
      "{'C': 0.13257113655901082}\n",
      "{'C': 0.13257113655901082}\n",
      "{'C': 0.13257113655901082}\n",
      "{'C': 0.15998587196060574}\n",
      "{'C': 0.15998587196060574}\n",
      "{'C': 0.15998587196060574}\n",
      "{'C': 0.19306977288832497}\n",
      "{'C': 0.19306977288832497}\n",
      "{'C': 0.19306977288832497}\n",
      "{'C': 0.23299518105153719}\n",
      "{'C': 0.23299518105153719}\n",
      "{'C': 0.23299518105153719}\n",
      "{'C': 0.28117686979742279}\n",
      "{'C': 0.28117686979742279}\n",
      "{'C': 0.28117686979742279}\n",
      "{'C': 0.33932217718953261}\n",
      "{'C': 0.33932217718953261}\n",
      "{'C': 0.33932217718953261}\n",
      "{'C': 0.40949150623804231}\n",
      "{'C': 0.40949150623804231}\n",
      "{'C': 0.40949150623804231}\n",
      "{'C': 0.49417133613238334}\n",
      "{'C': 0.49417133613238334}\n",
      "{'C': 0.49417133613238334}\n",
      "{'C': 0.59636233165946428}\n",
      "{'C': 0.59636233165946428}\n",
      "{'C': 0.59636233165946428}\n",
      "{'C': 0.71968567300115138}\n",
      "{'C': 0.71968567300115138}\n",
      "{'C': 0.71968567300115138}\n",
      "{'C': 0.86851137375135201}\n",
      "{'C': 0.86851137375135201}\n",
      "{'C': 0.86851137375135201}\n",
      "{'C': 1.0481131341546852}\n",
      "{'C': 1.0481131341546852}\n",
      "{'C': 1.0481131341546852}\n",
      "{'C': 1.2648552168552958}\n",
      "{'C': 1.2648552168552958}\n",
      "{'C': 1.2648552168552958}\n",
      "{'C': 1.5264179671752318}\n",
      "{'C': 1.5264179671752318}\n",
      "{'C': 1.5264179671752318}\n",
      "{'C': 1.8420699693267144}\n",
      "{'C': 1.8420699693267144}\n",
      "{'C': 1.8420699693267144}\n",
      "{'C': 2.2229964825261934}\n",
      "{'C': 2.2229964825261934}\n",
      "{'C': 2.2229964825261934}\n",
      "{'C': 2.6826957952797246}\n",
      "{'C': 2.6826957952797246}\n",
      "{'C': 2.6826957952797246}\n",
      "{'C': 3.2374575428176433}\n",
      "{'C': 3.2374575428176433}\n",
      "{'C': 3.2374575428176433}\n",
      "{'C': 3.906939937054613}\n",
      "{'C': 3.906939937054613}\n",
      "{'C': 3.906939937054613}\n",
      "{'C': 4.7148663634573893}\n",
      "{'C': 4.7148663634573893}\n",
      "{'C': 4.7148663634573893}\n",
      "{'C': 5.689866029018293}\n",
      "{'C': 5.689866029018293}\n",
      "{'C': 5.689866029018293}\n",
      "{'C': 6.8664884500429979}\n",
      "{'C': 6.8664884500429979}\n",
      "{'C': 6.8664884500429979}\n",
      "{'C': 8.2864277285468422}\n",
      "{'C': 8.2864277285468422}\n",
      "{'C': 8.2864277285468422}\n",
      "{'C': 10.0}\n",
      "{'C': 10.0}\n",
      "{'C': 10.0}\n"
     ]
    }
   ],
   "source": [
    "with np.errstate(all='ignore'):\n",
    "    param_grid_input = {'C': costs }\n",
    "    mglr = MultiClassLogisticRegression(eta=eta,iterations=iter_, C=0.02, optimization=\"BFGSBinaryLogisticRegression\")\n",
    "    gscv = GridSearchCV(cv= cv_object, estimator=mglr, param_grid= param_grid_input, scoring= \"accuracy\",refit=False)\n",
    "    gscv.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.40949150623804231}\n"
     ]
    }
   ],
   "source": [
    "print(gscv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our graph above, \"Determining Optimal Value of Regularization Term C\", we chose C to be ____ . GridSearchCV recommends we use _ for C. These are not too different so we are confident in our choice. While implementing GridSearchCV functionality for our classes was difficult and would require further implementations, we conclude that it would be a better choice (once implemented) than having to graph values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
